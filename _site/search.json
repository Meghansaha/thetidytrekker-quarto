[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My official title is a Data Scientist at the Prostate Cancer Clinical Trials Consortium (PCCTC) at the Memorial Sloan Kettering Cancer Center. I mostly use R to do daily data/programming/automation tasks to support Prostate Cancer research. I'm learning new things every day and always striving to further sharpen my data science and programming skills. My health research interest lies in mental and primary health care integration. My technical interest lies in automating data pipelines, data visualizations, and various user applications for data dissemination.In the past, I've worked for various government and public health entities and completed field training/internships in community mental health clinics and mental health research labs. I've had my share of working with various non-profit youth, educational, and social health clients as an evaluator as well. I'm an advocate for using mixed methods and automated data processing when possible and encourage the use of open-source responsibly. I plan to use this platform to share and collaborate with others on this professional journey.\nWhen I'm not working, I love to play video games, create art (especially rtistry), travel and spend time with my son, my Patterdale terrier, Patches, and my husband."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Tidy Trekker",
    "section": "",
    "text": "Let’s Talk About 2022\n\n\n\n\n\nYes. Hi. Hello. It’s been over a year since I’ve blogged. Let’s have a chat about 2022.\n\n\n\n\n\n\nJan 23, 2023\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nMaking Waves in ggplot: An Rtistry Tutorial\n\n\n\n\n\nAn Rtistry tutorial in ggplot about making waves in the R programming language.\n\n\n\n\n\n\nNov 24, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nThinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.\n\n\n\n\n\nRecently I’ve discovered the courage to dive into creative coding and generative aRt in R. Something that the R community calls “Rtistry.” My Rtistry journey so far has been an amazing and tranquil expedition into a world that seemed intimidating and scary on the outside but is honestly just a bottomless pit of fun and creativity on the inside.\n\n\n\n\n\n\nOct 19, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nWhere Have I Been?\n\n\n\n\n\nSo, it has been a good five months since I last made a post on The Tidy Trekker. I can assure you that I am alive and kicking!\n\n\n\n\n\n\nSep 7, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nMaking My First Shiny App\n\n\n\n\n\nThis year was the first time that I ever competed in a RStudio Shiny competition by submitting my first ever Shiny app, the TarotreadR.\n\n\n\n\n\n\nSep 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTallying “Checkbox” Survey Responses: R Walkthrough\n\n\n\n\n\nA guide on how to split up comma-separated survey responses in R using the grepl function in base R and various tidyverse packages . A common issue when processing “Checkbox” question types. This guide assumes that you understand how to load data into R, work with vectors, and work with basic control and loop functions.\n\n\n\n\n\n\nMar 18, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nMaking Dull* Dashboards in R (* Without Shiny)\n\n\n\n\n\nA walkthrough on taking prepared data and creating an interactive dashboard in R without the use of Shiny. The leaflet, flexdashboard, crosstalk, DT, and SummaryWidget packages will be used for this. This guide assumes that you are somewhat familiar with Rmarkdown and knitting markdown code.\n\n\n\n\n\n\nMar 15, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nHow I Became A “Not-Beginner” in R\n\n\n\n\n\nSome thoughts I have on being a “Not-beginner” (Intermediate) R programmer/Data Scientist.\n\n\n\n\n\n\nFeb 20, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nApply Family Notebook\n\n\n\n\n\nAn apply() function notebook to explore the different apply functions in Base R\n\n\n\n\n\n\nJan 23, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nOrdering Months\n\n\n\n\n\nA guide on ordering month “categories” chronologically and dealing with missing month data on a ggplot using Tidyverse packages and base R constants. This guide assumes that you understand how to load data into R, work with vectors, and are somewhat familiar with subsetting and ggplot2.\n\n\n\n\n\n\nJan 23, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nI Participated in My First Tidy Tuesday! : Project Recap\n\n\n\n\n\nI have always “hid in the shadows” with community challenges like this, but I took the leap and participated. Here’s a quick recap…\n\n\n\n\n\n\nJan 6, 2021\n\n\nMeghan Harris\n\n\n\n\n\n\n  \n\n\n\n\nSetting Intentions\n\n\n\n\n\nAs the fiasco that is the year 2020 comes to an end, I figured I’d set some intentions for the Tidy Trekker site moving forward.\n\n\n\n\n\n\nDec 31, 2020\n\n\nMeghan Harris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#data-frame-example",
    "href": "post/applyfamilynotes/apply-family.html#data-frame-example",
    "title": "Apply Family Notebook",
    "section": "Data Frame Example",
    "text": "Data Frame Example\n#Creating a mock data frame\nCity <- c(\"Buffalo\",\"NYC\",\"Seattle\",\"Austin\",\"Orlando\",\"Minneapolis\")\nCases <- c(500,2012,1876,635,4512,823)\nControls <- c(3426,5210,6753,5633,2013,1890)\n\nrecords <- data.frame(City,Cases,Controls, row.names = NULL)\n\nrecords\n##          City Cases Controls\n## 1     Buffalo   500     3426\n## 2         NYC  2012     5210\n## 3     Seattle  1876     6753\n## 4      Austin   635     5633\n## 5     Orlando  4512     2013\n## 6 Minneapolis   823     1890\n\nWe can use the apply function to calculate column sums…\n\n#Calculating the column sum of all applicable columns\napply(records[,2:3], 2,sum)\n##    Cases Controls \n##    10358    24925\n\n…Or row sums. (Note that these both produce vectors and that we subset the dataframe with [,2:3] to avoid R throwing an error for the first column that has strings in it. Can’t perform a mathematical function on character strings)\n\n#Calculating row sums\napply(records[,2:3], 1,sum)\n## [1] 3926 7222 8629 6268 6525 2713\n\nWe can name the vectors in one line of code with the names<- function:\n\n#Calculating row sums, but applying names from the \"City\" column\n`City Totals` <-  `names<-`(apply(records[,2:3], 1,sum), records$City)\n\n`City Totals`\n##     Buffalo         NYC     Seattle      Austin     Orlando Minneapolis \n##        3926        7222        8629        6268        6525        2713"
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#applying-statistic-more-complex-functions",
    "href": "post/applyfamilynotes/apply-family.html#applying-statistic-more-complex-functions",
    "title": "Apply Family Notebook",
    "section": "Applying Statistic (More Complex) Functions",
    "text": "Applying Statistic (More Complex) Functions\n\nWe can also do different statistical procedures based on each test’s requirement. Let’s do a T-test:\n\n#Making a mock data set for T-test\nEx1_grades <- c(67,53)\nEx2_grades <- c(90,89)\nEx3_grades <- c(89,95)\nEx4_grades <- c(95,87)\nEx5_grades <- c(100,99)\nStudent <- c(\"Student1\",\"Student2\")\n\nGrades <- tibble(Student,Ex1_grades,Ex2_grades,Ex3_grades,Ex4_grades,Ex5_grades)\n\nGrades\n## # A tibble: 2 x 6\n##   Student  Ex1_grades Ex2_grades Ex3_grades Ex4_grades Ex5_grades\n##   <chr>         <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n## 1 Student1         67         90         89         95        100\n## 2 Student2         53         89         95         87         99\n\nLet’s just say we want the P.value of one sample T-tests for each student and we want to place it in this dataset as a new column. What’s important to note is that arguments that would normally be passed through to your functions, go as separate arguments at the end of the apply function, after you declare which function you want used.\n\n#Turning off scientific notation formatting\noptions(scipen = 999)\n\n#Getting index of columns that end with the word \"grades\"\nGradeindexes <- grep((\"grades$\"),names(Grades))\n\n#Using apply to apply the t.test.\ntestresults <- apply(Grades[,Gradeindexes],1,t.test, alternative = \"two.sided\", conf.level = 0.95)\n\n#Using do.call to bind p values to the data set. Because results are in a list, we can use lapply and wrap it in \"as.vector\" for clean transfer into the dataframe\n\nGrades$`P Values` <- as.vector(format(do.call(rbind, lapply(testresults, function(x){x$p.value})), digits = 2))\n\nGrades\n## # A tibble: 2 x 7\n##   Student  Ex1_grades Ex2_grades Ex3_grades Ex4_grades Ex5_grades `P Values`\n##   <chr>         <dbl>      <dbl>      <dbl>      <dbl>      <dbl> <chr>     \n## 1 Student1         67         90         89         95        100 0.000098  \n## 2 Student2         53         89         95         87         99 0.000494"
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#data-frame-example-1",
    "href": "post/applyfamilynotes/apply-family.html#data-frame-example-1",
    "title": "Apply Family Notebook",
    "section": "Data Frame Example",
    "text": "Data Frame Example\n\nWe can use lapply to make changes to a data frame.\n\n# Changing column names in the \"records\" data frame to be all CAPS\n\nnames(records) <- lapply(names(records),str_to_upper)\n\nrecords\n##          CITY CASES CONTROLS\n## 1     Buffalo   500     3426\n## 2         NYC  2012     5210\n## 3     Seattle  1876     6753\n## 4      Austin   635     5633\n## 5     Orlando  4512     2013\n## 6 Minneapolis   823     1890"
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#list-example",
    "href": "post/applyfamilynotes/apply-family.html#list-example",
    "title": "Apply Family Notebook",
    "section": "List Example",
    "text": "List Example\n\nWe can use lapply to make changes to a list. Need to create a mock list.\n\n#Creating list from randomly sampled numbers, then adding names from the \"fruit\" constant that comes with base R\n\n#Setting a seed for reproducibility\nset.seed(555)\n\n#Generating the random sample of numbers\nstock <- sample(1:50,5)\n\n#Pulling the first five strings of the fruit constant from base R\nfruits <- fruit[1:5]\n\n#Coercing the sampled numbers into a list  \nInventory <- as.list(as.numeric(stock))\n\n#Setting the names of each randomly sampled number to each string in out fruits vector\nnames(Inventory) <- fruits\n\nInventory\n## $apple\n## [1] 42\n## \n## $apricot\n## [1] 49\n## \n## $avocado\n## [1] 24\n## \n## $banana\n## [1] 16\n## \n## $`bell pepper`\n## [1] 29\n\nWe can alter the list by adding 100 to each fruit’s count and assigning the result back to Inventory:\n\nInventory <- lapply(Inventory, function(x) (x+100))\n\nInventory\n## $apple\n## [1] 142\n## \n## $apricot\n## [1] 149\n## \n## $avocado\n## [1] 124\n## \n## $banana\n## [1] 116\n## \n## $`bell pepper`\n## [1] 129"
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#multiple-list-example",
    "href": "post/applyfamilynotes/apply-family.html#multiple-list-example",
    "title": "Apply Family Notebook",
    "section": "Multiple List Example",
    "text": "Multiple List Example\nWe can use mapply() to alter different elements within multiple lists, as oppose to lapply() which only works within one list. Let’s create multiple lists to test mapply() out.\n#Want to take these separate list, add th last name \"Smith\" to all the names, then get the final result in one place (a list)\n\nnames1 <- list(\"John\", \"Abigail\", \"Sam\",\"Judy\")\nnames2 <- list(\"Mary\", \"Lauri\", \"Gus\")\nnames3 <- list(\"Harold\", \"Peter\", \"Natalie\",\"Scott\",\"Fatima\")\n\n`Names List` <- mapply(function(x) paste(x,\"Smith\"), c(names1,names2,names3))\n\n`Names List`\n##  [1] \"John Smith\"    \"Abigail Smith\" \"Sam Smith\"     \"Judy Smith\"    \"Mary Smith\"    \"Lauri Smith\"   \"Gus Smith\"     \"Harold Smith\"  \"Peter Smith\"   \"Natalie Smith\" \"Scott Smith\"   \"Fatima Smith\""
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#multiple-vector-example",
    "href": "post/applyfamilynotes/apply-family.html#multiple-vector-example",
    "title": "Apply Family Notebook",
    "section": "Multiple Vector Example",
    "text": "Multiple Vector Example\nMapply() can be used to vectorize function results from multiple vectors.\nLet’s same we have vectors of numbers and we want to know the mean of all of them separately:\n\n#Making mock vectors, setting a seed for reproducibility.\nset.seed(321)\n\n#Assigning the vectors\nvector1 <- sample(1:100,12)\nvector2 <- sample(1:100,5)\nvector3 <- sample(1:100,9)\n\nvector1\n##  [1] 54 77 88 80 58 17 47 11 25 31 82 79\nvector2\n## [1] 98 75 31 82 36\nvector3\n## [1] 78 87 34 84  4 48 51 80 13\n\nBecause we want summaries (the mean) of each vector, we can use the list() function instead of the c() function. We can use the MoreArgs argument to pass the trim argument to the mean() function. By default, this is set at zero, but passing it through to demonstrate.\n\n#Calculating the mean of each vector\nvectormeans <-mapply(mean,list(vector1,vector2,vector3), MoreArgs = list(trim = 0))\n\n#Setting names to the results\nnames(vectormeans) <- c(\"vector1\",\"vector2\",\"vector3\")\n\nvectormeans\n##  vector1  vector2  vector3 \n## 54.08333 64.40000 53.22222"
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#nested-list-example",
    "href": "post/applyfamilynotes/apply-family.html#nested-list-example",
    "title": "Apply Family Notebook",
    "section": "Nested list example",
    "text": "Nested list example\nLet’s say we have a list of cities that have a list of restaurant types embedded in them:\nRestaurantdata <- list(\"Buffalo\" = list(\"italian\",\"mexican\",\"japanese\",\"puerto rican\"),\n                       \"Seattle\" = list(\"japanese\",\"chinese\",\"southern\",\"steakhouse\"),\n                       \"Miami\" = list(\"seafood\",\"cuban\",\"italian\",\"polish\"))\n\nRestaurantdata\n## $Buffalo\n## $Buffalo[[1]]\n## [1] \"italian\"\n## \n## $Buffalo[[2]]\n## [1] \"mexican\"\n## \n## $Buffalo[[3]]\n## [1] \"japanese\"\n## \n## $Buffalo[[4]]\n## [1] \"puerto rican\"\n## \n## \n## $Seattle\n## $Seattle[[1]]\n## [1] \"japanese\"\n## \n## $Seattle[[2]]\n## [1] \"chinese\"\n## \n## $Seattle[[3]]\n## [1] \"southern\"\n## \n## $Seattle[[4]]\n## [1] \"steakhouse\"\n## \n## \n## $Miami\n## $Miami[[1]]\n## [1] \"seafood\"\n## \n## $Miami[[2]]\n## [1] \"cuban\"\n## \n## $Miami[[3]]\n## [1] \"italian\"\n## \n## $Miami[[4]]\n## [1] \"polish\"\n\nWe want to change all of the elements so that each restaurant type is capitalized. We can do this with either the tools or stringr packages. I’ll use the stringr package for this example. Note that the \"replace\" option in the how argument will actually alter the Restaurantdata list, but in order to save it as such, we have to assign in back to the Restaurantdata object.\n\nRestaurantdata <- rapply(Restaurantdata,stringr::str_to_title,how = \"replace\")\n\n\nRestaurantdata\n## $Buffalo\n## $Buffalo[[1]]\n## [1] \"Italian\"\n## \n## $Buffalo[[2]]\n## [1] \"Mexican\"\n## \n## $Buffalo[[3]]\n## [1] \"Japanese\"\n## \n## $Buffalo[[4]]\n## [1] \"Puerto Rican\"\n## \n## \n## $Seattle\n## $Seattle[[1]]\n## [1] \"Japanese\"\n## \n## $Seattle[[2]]\n## [1] \"Chinese\"\n## \n## $Seattle[[3]]\n## [1] \"Southern\"\n## \n## $Seattle[[4]]\n## [1] \"Steakhouse\"\n## \n## \n## $Miami\n## $Miami[[1]]\n## [1] \"Seafood\"\n## \n## $Miami[[2]]\n## [1] \"Cuban\"\n## \n## $Miami[[3]]\n## [1] \"Italian\"\n## \n## $Miami[[4]]\n## [1] \"Polish\"\n\nWe can also get a vector of our results by using the unlist option in the how function instead. Let’s add the word “restaurants” to each of these elements then unlist the object to place it in a vector.\n\nRestaurantvector <- rapply(Restaurantdata,function(x) paste(x,\"restaurants\"),how = \"unlist\")\n\n\nRestaurantvector\n##                   Buffalo1                   Buffalo2                   Buffalo3                   Buffalo4                   Seattle1                   Seattle2                   Seattle3                   Seattle4                     Miami1                     Miami2                     Miami3                     Miami4 \n##      \"Italian restaurants\"      \"Mexican restaurants\"     \"Japanese restaurants\" \"Puerto Rican restaurants\"     \"Japanese restaurants\"      \"Chinese restaurants\"     \"Southern restaurants\"   \"Steakhouse restaurants\"      \"Seafood restaurants\"        \"Cuban restaurants\"      \"Italian restaurants\"       \"Polish restaurants\""
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#data-frame-example-2",
    "href": "post/applyfamilynotes/apply-family.html#data-frame-example-2",
    "title": "Apply Family Notebook",
    "section": "Data Frame Example",
    "text": "Data Frame Example\nWe can use tapply() to apply a function across a column in a dataframe. Let’s make a simple data frame:\n# Setting a seed for reproducibility\nset.seed(789)\n\nTeams <- c(\"UK\",\"USA\",\"Egypt\",\"Ireland\",\"UK\",\"USA\",\"USA\")\nSeconds <- runif(length(Teams), min=30, max = 240)\nRunners <- data.frame(Teams,Seconds)\n\nRunners\n##     Teams   Seconds\n## 1      UK 176.97782\n## 2     USA  49.63476\n## 3   Egypt  32.49623\n## 4 Ireland 154.23733\n## 5      UK 133.35138\n## 6     USA  34.23435\n## 7     USA 150.25773\n\nLet’s say we want to calculate the average time (seconds) for each Team. We can use tapply() for this.\n\nRunner_means <- tapply(Runners$Seconds, Runners$Teams, mean)\n\nRunner_means\n##     Egypt   Ireland        UK       USA \n##  32.49623 154.23733 155.16460  78.04228\n\nThe results are stored into an array. If a new table is desired it can be manipulated to do so:\n\nRunners_summary <- data.frame(Team = names(Runner_means), Mean =Runner_means, row.names = NULL)\n\nRunners_summary\n##      Team      Mean\n## 1   Egypt  32.49623\n## 2 Ireland 154.23733\n## 3      UK 155.16460\n## 4     USA  78.04228"
  },
  {
    "objectID": "post/applyfamilynotes/apply-family.html#jagged-vectors-example",
    "href": "post/applyfamilynotes/apply-family.html#jagged-vectors-example",
    "title": "Apply Family Notebook",
    "section": "Jagged Vectors Example",
    "text": "Jagged Vectors Example\n\nWe can use tapply() to also preform tasks across multiple vectors of different lengths as long as the amount of factors matches the amount of the elements overall in each vector\n\n#Create a list of factors \nNames <- c(\"Meghan\",\"Gus\",\"Jennifer\",\"Gus\",\"Jennifer\",\"Natalie\",\"Meghan\",\"Jennifer\",\"Gus\",\"Natalie\")\n\nScores1 <- c(90,67,88,99,100)\nScores2 <- c(99,99,78)\nScores3 <- c(100,78)\n\n#Placing them together gives us a vector that has a length of 10. Same as the \"Names\" vector\nTest_scores <- c(Scores1,Scores2,Scores3)\n\n#We can now apply the same function across this vectors and get summarized results\nTest_averages <- tapply(Test_scores,Names,mean)\n\nTest_averages\n##      Gus Jennifer   Meghan  Natalie \n## 88.66667 88.66667 94.50000 88.50000"
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html",
    "href": "post/dull-dashboards/dull-dashboards.html",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "",
    "text": "Don’t have time to read and just want the source code? Click Here\nThe Problem: You want to put cleaned/prepared data into an interactive dashboard but you can’t or don’t want to code with Shiny at the moment.\nThe Fix: You need to use the flexdashboard package and create a markdown document instead."
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#getting-started",
    "href": "post/dull-dashboards/dull-dashboards.html#getting-started",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Getting Started:",
    "text": "Getting Started:\nTo start, we’ll look at a cleaned and prepped dataset that includes a mix of categorical and geographic variables. The data is publicly sourced from the New York State Department of Mental Health and contains information for New York State’s Local Mental Health Programs.\nFor this example, I’ve already cleaned and prepped the set. If you’re interested, you can view the cleaning script and geoprocessing* script. Our main purpose for this project is to create a dashboard with a map, filters for the map, and a data table that let’s us view selected data points.\n*The geoprocessing coding in this script came from Dmitry Kisler’s article “OSM Nominatim with R: getting Location’s Geo-coordinates by its Address” Check it out to learn more about the process if you’d like!\nIf you’d like to follow along within the R project files you can download them from the Tidy Trekker Github Repository. Be sure to fully extract the ZIP. folders for proper access. You can also download the data set used and load it into your own session for practice.\n\n\n\nDirect download link for this post’s example data"
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#creating-the-markdown-file",
    "href": "post/dull-dashboards/dull-dashboards.html#creating-the-markdown-file",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Creating the Markdown File",
    "text": "Creating the Markdown File\nIn order to create the dashboard, we need to write out code in an R Markdown file (.RMD) instead of an R Script (.R). If you don’t have the flexdashboard package installed already you’ll want to do that first:\n\n# Installing the flexdashboard package===\ninstall.packages(\"flexdashboard\")                                                    \n\nAfter installing flexdashboard, you should be able to create a new markdown document with the flexdashboard template. You can do so by going to create a new markdown document like you normally would…\n\n\n\n\nCreating a new markdown document in the R Studio IDE\n\n\n\n…but instead of accepting the default document settings, click on the “From Template” option in the left pane, select the “Flex Dashboard” template in the right pane, and finally click “OK” at the bottom:\n\n\n\n\nR Markdown Template Menu in R Studio IDE\n\n\n\nYou’ll now see a pre-generated Flex Dashboard markdown script! You can use this for practice. If you have some time and are interested, play around with the syntax and get a feel for how changes to this template will affect the final knitted document. You can read up more about Flex Dashboard basics here. If you want more information about different layout options you can find them here. For this guide, we’ll be using the “Chart Stack (fill)” layout."
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#setting-the-yaml",
    "href": "post/dull-dashboards/dull-dashboards.html#setting-the-yaml",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Setting the YAML",
    "text": "Setting the YAML\nThe first part of the markdown code that we see is the YAML (Yet Another Markup Language or YAML Ain’t Markup Language) Header. Without getting bogged down with an explanation, this is basically a special block of code that’s going to tell R how to process our markdown document. It’s the “magic battery” that’s needed to power the translation process that converts code to pretty documents. For this project our YAML looks like this:\n\n---\ntitle: \"New York State's (Brooklyn,NY) Local Mental Health Programs\"\noutput:\n  flexdashboard::flex_dashboard:\n    vertical_layout: fill\n    source_code: embed\n    css: scripts/nycmhstyle.css\n---\n\n\nThe Components of this YAML:\n\nTitle: This is the string that will show up in the top left corner of your dashboard.\nOutput: This is where the document type is set. You can set options for this on the same line, or underneath it (shown here.)\nOutput Options: In this YAML, our options are underneath the “output” line and are indented. The indents are extremely important. If these options aren’t indented in a hierarchical (ordered) fashion, it will not work! Let’s look at them further:\n\n\n\n Please be mindful that the YAML output options are properly ordered and indented! You’ll know if it’s not because you will get an error.\n\n\n\nflexdashboard::flex_dashboard: Note how this is tabbed or indented only once under the output option. This tells R we want this document processed as a Flex Dashboard.\nvertical_layout: fill: Also note how it’s indented/tabbed again. This tells R that these are options that we want applied to this dashboard. This option tells R that we want the dashboard to fill the page vertically. This option can also be set to “scroll.” You can always play around with it and see which is best for your needs.\nsource_code: embed: This option tells R that we want to share our source code on the dashboard. This will appear as a button in the top right corner of the dashboard. If we have source code elsewhere on the internet (like GitHub) you can replace “embed” with the actual URL as a string (with quotation marks - “https://github.com/user/code”)\ncss: scripts/nycmhstyle.css: This option tells R that we want to use an external css file to change the appearance of our flexdashboard. This file is included on the GitHub repository if you’re interested."
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#configuring-the-setup-code-chunk",
    "href": "post/dull-dashboards/dull-dashboards.html#configuring-the-setup-code-chunk",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Configuring the “Setup” Code Chunk",
    "text": "Configuring the “Setup” Code Chunk\nNow that the YAML is set, we need to load in all the packages we’ll use and our data.\n\n\n#Library load in.#\nlibrary(flexdashboard)\n\nWarning: package 'flexdashboard' was built under R version 4.1.3\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.1.3\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   1.0.1 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.3.0      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.1.3\n\nlibrary(leafem)\n\nWarning: package 'leafem' was built under R version 4.1.3\n\nlibrary(crosstalk)\nlibrary(DT)\nlibrary(summarywidget)\nlibrary(raster)\n\nWarning: package 'raster' was built under R version 4.1.3\n\n\nLoading required package: sp\n\n\nWarning: package 'sp' was built under R version 4.1.3\n\n\n\nAttaching package: 'raster'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\nThere’s a good amount of packages here. I’ve already mentioned flexdashboard, leaflet, crosstalk, DT, and summary widgets, but there’s a few more needed for our particular example: - knitr: Helps with markdown/report generation - leafem: Gives extensions and additional options for the leaflet (map) package. - tidyverse: Group of packages used to aid with data wrangling/manipulation - raster: Helps with geographical data analysis and modelling.\n\nAfter loading in all of the libraries, we can now load in our data. The data for this project is stored as an .RDS object. The original csv file was converted to an .RDS object to ensure reproducibility and maintain integrity of the data’s structure. The readRDS() function will automatically open the object and restore it as a data frame with all original components intact. This code stores the data frame as MHprograms\n\n\n#Loading the datasets into the environment.#\n#MH programs with geocodes#\n\nMHprograms <- readRDS(\"data/mhprogramsnyc.RDS\")\n\n\nNow that our data is in the environment, we can use the crosstalk package to translate our data into a shared data object. This will allow interactivity between our dashboard components.\n\n#Converting data frames into shared data objects with crosstalk. This will allow the map, filters, and tables to be interactive and dynamic.#\nSharedgeodata <- SharedData$new(MHprograms)"
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#thinking-about-dashboard-components-and-real-estate",
    "href": "post/dull-dashboards/dull-dashboards.html#thinking-about-dashboard-components-and-real-estate",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Thinking about Dashboard Components and Real Estate",
    "text": "Thinking about Dashboard Components and Real Estate\nWhen creating our dashboard we must think about the type of data we have and how that data will fill the space on our dashboard. Before thinking about where things will go, we need to think about what would be useful to have on our dashboard. There’s 17 variables in the set, but for the sake of this example let’s focus on the following:\n\n Agency Name Program Name Agency Phone Program Address 1 Program State Program Zip Program Category Program Subcategory Populations Served\n\n\nThese variables should be just enough to give the user basic description information about the programs in the data set. Keeping this in mind, we might now start to think about what would be useful to add to our dashboard and map. How about the following?\n\nOn the map itself:\n\nA “Home Button” - A button that will snap our map view back to our data if we go wondering off in the map.\nConditional Formatting - for map icons based on the program category (i.e. different icons based on the program type)\n\nOn the dashboard, separate of the map:\n\nReactive Values - Values that change based on the data that’s shown on the map. For this example we’ll do a simple “count” of all mental health programs that were found.\nA Simple Data Table - A simple table that can display information about all programs shown on the map.\nFilters - Filters that will limit what locations are seen on the map based on options that the user selects.\n\n When you start to work with flexdashboard more, you’ll learn about the quirks of layout templates and will begin to see what’s possible. It’s recommended to take time to understand how you want your components to work and look together. For some people this may mean taking the time to draw on paper or making a “mock” dashboard in MS Paint. Whatever your method of planning, just make sure you have a plan! Here’s our mock plan drawn up for our dashboard:\n\n\n\n\nMock-up Plan for our NY Mental Health Dashboard"
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#setting-up-our-first-column-and-map",
    "href": "post/dull-dashboards/dull-dashboards.html#setting-up-our-first-column-and-map",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Setting up our First Column and Map",
    "text": "Setting up our First Column and Map\nSo looking at our mock-up, we just need two columns. One on the left with just a map and one of the right with three modules and some text at the bottom. To set flexdashboard columns in markdown you need to put the following “outside” of r code chunks in the regular markdown editor:\n\nColumn {data-width=550}\n-----------------------------------------------------------------------\n\n### \n\nBecause we want the first column to be filled with our map, we’ll set the width to be a bit wider than the second column. The series of dashes (—) under the “column” option along with the three hashes (#) let’s R know to place everything underneath those hashes in it’s own column within it’s own box. The hashes are also where you can place the title of your box within your column. This one will be left blank to maximize the space for the map.\nNow that the column is initialized, we can place more r code chunks underneath it to get our data in. Our first chunk will be for our map. Recall that we designate the beginning of R code chunks with three back-ticks (aka: Grave Accents) and square brackets with options and end them with an additional three back-ticks at the bottom of the chunk:\n```{r map}\n\n```"
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#creating-the-home-button-bounding-box-and-map-center",
    "href": "post/dull-dashboards/dull-dashboards.html#creating-the-home-button-bounding-box-and-map-center",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Creating the Home Button, Bounding Box, and Map Center",
    "text": "Creating the Home Button, Bounding Box, and Map Center\nLet’s recall what we wanted on our map. A home button and conditional formatting for program types. Let’s code for our home button to start. Say we want an actual image of a home. We’ve gone ahead and found a url of one. To create this button, we’ll need to use the HTML function in the htmltools package to use some HTML for this:\n\n\n#Pulling a \"Home\" icon I've hosted on the internet===\nHomeimg <- htmltools::HTML(paste(\"<img src='https://i.ibb.co/D8tSw9q/Homebutton.png' width = '15', height '15', title = 'Home'>\"))\n\n#Naming the home image for the addhomebutton function in the leaflet map===\nHome <- list(\"Home\" = Homeimg)\n\n\nThis code will show us a pretty home icon, but we’ll need to feed the button the bounding box (bbox) so that it knows where to set the map’s view when it’s clicked on. We can do this easily with the make_bbox function in the ggmap package as long as our data has longitude and latitude columns with numeric values (Ours does!)\n\n\n#Generating a bounding box of the entire data set. This will act as our \"homebase\" to zoom back to when the home button is clicked on===\nhomebase <- ggmap::make_bbox(lon, lat, data = MHprograms)\n\n\nWe may also want to designate a “center” for our map that is first shown when the dashboard is open. We can do this by using the coordinates and proj4strings functions from the sp package, The extent function from the raster package, and the as function that comes from the methods package that usually loads in with base R.\n\n\n# Retrieving the center point for the map data to set the \"home\" view===\ncoordinates(MHprograms) <- ~lon+lat\nproj4string(MHprograms) <- \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\nmapcenter <- coordinates(as(extent(MHprograms), \"SpatialPolygons\"))\n\n\nThe options set in the proj4string line is referring to the Coordinate Reference System that R uses to create our maps. You can learn more about it with some reading material provided by the National Center for Ecological Analysis and Synthesis if you’re interested."
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#creating-the-actual-map",
    "href": "post/dull-dashboards/dull-dashboards.html#creating-the-actual-map",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Creating the Actual Map",
    "text": "Creating the Actual Map\nNow we can finally get our map into the column. The code is as follows:\n\n#Creating the leaflet map from the shared geo data object===\nSharedgeodata %>%\n  leaflet() %>%\n  addProviderTiles(providers$OpenStreetMap) %>%\n  addAwesomeMarkers(\n    popup = ~paste0(\n      \"<b><h3>\",MHprograms$`Agency Name`, \"</h3></b><br>\",\n      \"<h4>\",MHprograms$`Program Name`,\"</h4><br>\",\n      \"Phone: \",MHprograms$`Agency Phone`, \"<br>\",\n      MHprograms$`Program Address 1`, \"<br>\",\n      MHprograms$City, MHprograms$`Program State`, MHprograms$`Program Zip`), \n    icon = awesomeIcons(\n      library = \"fa\",\n      icon = ifelse(\n        test = MHprograms$`Program Category Description` == \"Outpatient\", \n        yes = \"fa-stethoscope\",\n        no = ifelse(\n        test = MHprograms$`Program Category Description` == \"Inpatient\", \n        yes =  \"fa-bed\",\n        no = ifelse(\n        test = MHprograms$`Program Category Description` == \"Emergency\", \n        yes = \"fa-ambulance\",\n        no = \"fa-users\"\n      ))),\n      iconColor = \"#ffffff\",\n      markerColor = \"darkpurple\")) %>%\n  addHomeButton(ext = homebase, \n                group = Home,\n                position = \"topright\") %>%\n  setView(lng = mapcenter[1] , lat = mapcenter[2], zoom = 12)\n\n\nThis part of the code starts out by using a pipe operator (%>%) to connect our leaflet map functions to our shared data object Sharedgeodata. Let’s break down the main functions we have here:\n\n\nleaflet(): Initiates our leaflet map widget.\naddProviderTiles(): This draws on actual map tiles to create a basemap. In this example we are using OpenStreetMaps. Being as this is Leaflet’s default, we could replace this with addTiles() with no options set inside and it would produce the same result.\naddAwesomeMarkers(): This allows us to add markers on our map with custom color and icons on them.\n\nArguments within the addAwesomeMarkers() function:\n\npopup = This is the text that will popup when a marker is clicked on. You can paste data from the original data set that was converted into a shared data object (in this case, the MHprograms set). In this example, we’re pasting the Agency Name, Program Name, Phone Number and Address associated with each marker (observation) in the data set. I’m utilizing HTML here to assist with formatting because I did not spend years learning it on Myspace for nothing. If you’d like to learn more about HTML basics you can do so here.\nicon = These are the options that we set for our icons that show up on our marker. For this example we are using an additional function awesomeIcons() which will allow us to further customize the icons that are shown on our markers. In this function we have two additional arguments (library= & icon=.) In this example we set library= to \"fa\" because we are using the Font Awesome library. Our icon argument is set to an ifelse function because we want the icons to change conditionally based on the type of program we are displaying at the marker.\niconcolor = & markercolor = Self-explanatory. Changes the color of the icon in the marker and the marker itself.\naddHomeButton(): This allows us to add a “home/zoom to” button onto our map. Recall that we did some work for this already. This is where we fill in details to make the magic happen.\n\n Arguments within the addHomeButton() function:\n\next = This is the extent variable or bounding box that you’d like the home button to take the user to when clicked on. We already created our bounding box earlier and named it homebase.\ngroup = This is the group or layer that will be shown in the home button on the map. A character string is expected here. Because we wish to use the home icon for our button that we set up earlier, we use the name we set it to earlier (home). This tells R to place our picture there instead of the text “home.”\nposition = Self-explanatory. This lets us choose where our home button will be displayed on the map. The options are topleft, topright, bottomleft, or bottomright. The default is bottomright.\nsetView(): This allows us to determine the view that is seen when the map is open. lng and lat refer to longitude and latitude respectively. We can pull these values from the object we’ve already created called mapcenter. We do this by subsetting the first and second elements of mapcenter which is our longitude and lattiude values for the center of all of our data. We can also set the zoom level with the zoom argument. The zoom scale for Leaflet goes from 0-18 with 0 being a view of the entire world and 18 being the closest you can get to a “street” view."
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#setting-up-our-second-column",
    "href": "post/dull-dashboards/dull-dashboards.html#setting-up-our-second-column",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Setting up our Second Column",
    "text": "Setting up our Second Column\n\nReactive Summary Count of Records\nGreat! Our first column is done. Now we need to create our second (right) column. Looking at our mock-up will let us know what order we wanted our elements to appear in. The first element is a reactive count of the amount of locations that are present on our map at any given time. Because we are creating a new column, we can repeat the same coding header we used for the last column with some tweaks to the sizing. We’ll make this column smaller because we want the map to take up most of the space on the page. We’ll get our reactive count by using the summarywidget function:\n\n#Start of the second Column\nColumn {data-width=450}\n-----------------------------------------------------------------------\n\n### **Total Locations Found:** {data-height=70}\n<center><h4><font color=\"#593869\"><b>`r summarywidget(Sharedgeodata, statistic='count', digits=0)`</b></font></h4></center>\n\n\nRecall that we tell R that we want to create a box within our column with the three hashes (###). This time we will give it the name “Total Location Found:” and we will bold it by wrapping it in two asterisks (If you need a quick cheat sheet for R Markdown formatting, you can find one here!) We will also set the height of this box which is similar to setting widths of columns.\nUsing some HTML tags, we can center and change the size and color of the inline code we are using. In R Markdown, if you would like to insert R code to be evaluated within the lines of your markdown document, you can use backticks with an “r” inside of it before the code you wish to evaluate. Let’s pull this inline code out and look at what’s happening really quick:\n\n\n`r summarywidget(Sharedgeodata, statistic='count', digits=0)`\n\n\nThe first backtick and “r” let’s R know that it’s about to evaluate whatever comes next and then print that result in the markdown document as if it’s not code, meaning it’ll just print out the result. The summarywidget function is interacting with our shared data object Sharedgeodata to grab our statistic of choice (a count) for the records that are available at any given time. Because this is the same shared data object that’s used in our map (and our data table and filters that we’ll create in a moment), whenever anything on the dashboard is changed, this value with reactively change as well. Finally, we also set the digits in this function to zero to avoid any decimal numbers.\n\n\n\nCreating a Simple Reactive Data Table\nWe can now move on to our next box which is a data table. We set it up similarly with the hashes and set the data height like we did for the previous box. To display a data table, we’ll now use the datatable function from the DT package. Note that this code will go into it’s own R code chunk named datatable.\n\n\n### **Program Information:** {data-height=200}\n\n```{r datatable}\nSharedgeodata %>% \n  datatable(\n    rownames = FALSE,  \n    style = \"bootstrap\",\n    class = \"compact\",\n    selection = \"multiple\",\n    options = list(\n      dom = \"tip\", \n      columnDefs = list(\n        list(width = '50%',\n          visible = FALSE,\n          targets = c(0,4:13,15:16))),\n    colnames = c(\n      \"Location Name\" = \"Program Name\",\n      \"Ages Served\" = \"Populations Served\",\n      \"Phone Number\" = \"Program Phone\",\n      \"Address\" = \"Complete Address\"\n      ))) \n```\n\nAfter using the pipe operator (%>%) to link to our shared data object Sharedgeodata, we use the datatable() function to make our table. There’s a lot going on here so let’s break it down:\n\n\nrownames = Simply the option for controlling the row names on the table. You can also set this to a character vector with the strings desired in it.\nstyle = The table style you’d like. You can learn more about the DT table styles here. Sources say that R can only utilize the options \"default\", \"bootstrap\", and \"bootstrap4\" at this time.\nselection = Controls how many rows/columns the user can select at a time. Options are \"multiple\", \"single\", and \"none\".\noptions = This contains a list of options used to initialize our data table. Let’s break the options down further:\n\n Arguments within options=:\n\ndom = This is the Document Object Model for the table and can be seen in JavaScript usually. This is how we can get things like the actual table, pagination control, and more. For our example we’ve set it to “tip”. This translates to t-“table”, i-“table information summary”, and p-“pagination control”. These options can be placed in any order you wish, but it will affect the order in which it’s layered onto the dashboard. In this example our table gets printed first, then our information summary with the pagination buttons at the very bottom. To learn more about possible DOM options, you can read more about them here.\ncolumnDefs = These are the options we apply specifically to our columns. Note that R needs this info within a nested list (a list within a list) to interpret it properly. width = simply controls the sizing of the columns in the table. You can play around with this to see which percentage is better for your needs. visible = tells R whether or not we want specific columns to be shown or not. We have ours set to FALSE because we want R to hide most of our columns. targets = is where we tell R which columns to hide with the column index numbers. Note that the indexing within datatable arguments are zero-based. Meaning that the first column in the dataset is considered index “0” instead of index “1” like R normally does.\ncolnames = Self-explanatory. This is a vector of names that you want shown in the table. The function expects the name you want displayed first set to the name of the column in the actual data set."
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#setting-up-reactive-filters",
    "href": "post/dull-dashboards/dull-dashboards.html#setting-up-reactive-filters",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Setting up Reactive Filters",
    "text": "Setting up Reactive Filters\n\nCreating a Checkbox Filter\nWe can finally set up our filters that will interact with our map and data table. We do so by creating a new box in our dashboard. We’ll bold its title and name it “Map Filters:”.\n\n\n### **Map Filters:**\n<center>\n\n```{r mapfilters}\n\n  filter_checkbox(\n    id = \"Program Category Description\",\n    label = \"Program Type\",\n    sharedData = Sharedgeodata,\n    group = ~`Program Category Description`,\n    inline = TRUE\n  )\n\n```\n</center>\n\nWe’ve wrapped the R code block in some HTML tags to center the filter within the box. Using the crosstalk package, we can create our filter with the filter_checkbox() function.\nArguments within filter_Checkbox():\nid = Reserved for element IDs. In this example we use the dataset’s column name that we want to filter on.\nlabel = Expects a character string that will be shown on the actual dashboard.\nsharedData = Where we put our shared data object we created (the same object that’s used in the datatable and map).\ngroup = Is where we put a formula that will be used for filling the data within this particular filter. Normally this is just a column that’s either filled with factors or character vectors. The tilde (~) let’s R know it needs to interpret it as a function. In our example, our data is already cleaned and prepared for use but in other cases where special selections need to be made, you can enter more complicated functions on a column to achieve your desired result.\ninline = Tells R whether to display the filter options horizontally (TRUE) or vertically (FALSE).\n\n\n\nCreating Selection Filters\n\nSelection filters are very similar in it’s syntax but will display the options in a drop-down menu instead:\n\n```{r mapfilters2}\n\n  filter_select(\n    id = \"Program Subcategory Description\",\n    label = \"Program Setting\",\n    sharedData = Sharedgeodata,\n    group = ~`Program Subcategory Description`,\n    multiple = TRUE\n  )\n\n```\n```{r mapfilters3}\n\n  filter_select(\n    id = \"Populations Served\",\n    label = \"Ages Served\",\n    sharedData = Sharedgeodata,\n    group = ~`Populations Served`,\n    multiple = FALSE\n  )\n\n```\n\nNote that we have two more R code chunks. Both of these filters could go in one code chunk, but I personally like to break my filters up into different chunks for easier debugging if things go wrong. It’s a matter of preference. The code for the filter_select() function is very similar to the filter_checkbox() function we just used. The only addition is that of the multiple = argument. This dictates whether the user can select multiple options at once. For example purposes, we’ve set our Program Setting filter to TRUE for this (meaning we can select multiple options) and we’ve set our Ages Served filter to FALSE (meaning we can only select one option at a time)."
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#finishing-up-the-dashboard",
    "href": "post/dull-dashboards/dull-dashboards.html#finishing-up-the-dashboard",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Finishing up the Dashboard",
    "text": "Finishing up the Dashboard\nTo finish up this dashboard, you can add some brief information about the data and it’s source. I feel this code chunk is a perfect example of some of flexdashboard limitations:\n\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\nData provided by the <b><a href = \"https://omh.ny.gov/\">New York State Office of Mental Health</a></b> and found publicly on <b><a href = \"https://data.ny.gov/Human-Services/Local-Mental-Health-Programs/6nvr-tbv8\">DATA.NY.GOV</a></b>\n<br>\nExample created by <b><a href = \"https://twitter.com/meghansharris\"> Meghan Harris</a></b> with the <b><a href=\"https://rmarkdown.rstudio.com/flexdashboard/\">flexdashboard</a></b>, <b><a href= https://rstudio.github.io/crosstalk/\"> Crosstalk</a></b>, <b><a href=\"https://kent37.github.io/summarywidget/index.html\"> SummaryWidget</a></b>, and <b><a href =\"https://rstudio.github.io/DT/\">DT</a></b> packages. \n\n\nYou can see the finished dashboard in action on the github repo here.\n\n\n\n\nFinished FlexDashboard"
  },
  {
    "objectID": "post/dull-dashboards/dull-dashboards.html#limitations",
    "href": "post/dull-dashboards/dull-dashboards.html#limitations",
    "title": "Making Dull* Dashboards in R (* Without Shiny)",
    "section": "Limitations",
    "text": "Limitations\n\nWhile flexdashboard is great because it allows you to whip up some great dashboards without shiny, you are confined to the environment set by the template. It calls for some “hacking” sometimes and if you want your dashboard appearance to be customized (color/themes), some knowledge in CSS/HTML will make it an easier process. You can also use preset themes to change the appearance of your dashboard as well.\nSizing is also a tricky issue with flexdashboards. Because the template uses a browser-based flexbox engine, the sizing will vary based on things like browser window and user monitor size. You’ll also have to consider mobile layouts for viewing the dashboard on mobile devices if this is of interest. Despite the limitations, flexdashboards can be pretty useful and a less intimidating way for those without experience in Shiny to start making dashboards in R.\nLastly, special thanks to Matt Dray for his presentation and work on these packages! He’s the reason I found flexdashboards!\nHave you ever used flexdashboards before? Have any thoughts or suggestions? Know of a better solution or way to make this script more efficient? Feel free to contact me directly! Respectful discourse towards efficient solutions or new insights is always welcomed!"
  },
  {
    "objectID": "post/how-i-became-a-not-beginner-in-r/not-beginner.html",
    "href": "post/how-i-became-a-not-beginner-in-r/not-beginner.html",
    "title": "How I Became A “Not-Beginner” in R",
    "section": "",
    "text": "I’ve been in my current position as a Data Integration Specialist for about nine months now and I absolutely love it! I have achieved a lot of professional development that intertwined perfectly with my actual position. I’ve learned how to clean data more efficiently, deal with multiple data sources, and even learned how to create automated reports and dashboards in R and R Markdown.\nMore recently, I’ve had a local opioid dashboard (that I created in R) featured on our local news station. Like on actual TV (yeah, I’m still in shock about that one.)\nIt was then that I realized: I’m not a beginner in R anymore\n\n\n\n\n“Me realizing I’m not a beginner anymore”\n\n\n\nThat realization made me think about things though. It was clear that I wasn’t a beginner anymore, but if I wasn’t a beginner, what was I? I suddenly had an existential moment where I realized I needed to have a conversation with Data Science and R programming (as if they were actual people) and ask them about our relationship status. I had to ask them:\n\n“What are we?”\n\n\nI’m definitely not an expert, not by a long shot, but I can do A LOT of really essential things in R now. Whenever I’m interacting with others in the field, or assisting others with their programming, I always felt I needed to give a disclaimer that “I’m still learning.” People would then kind of roll their eyes at me after I explain things by telling me I’m a “wizard” or “expert” and to stop being modest. So that made me think about it. How should I be labeling myself? The more I tried to put a label on my data science/R programming proficiency, the more I realized that I’m currently in some weird gray area of my journey; and honestly? I don’t think it’s talked about enough.\nThere’s a wide range of resources out there for aspiring R programmers and data scientists. The first thing everyone tells you to do (myself included) is to start reading R for Data Science and to just practice. There’s also a wealth of information about more advanced R topics like modeling and advanced Shiny app development. I remember longing to join discussions about these things at the RStudio::Global conference this year, but I ultimately felt like I wasn’t quite ready to join those conversations yet. This made me feel like the space wasn’t for me and that I couldn’t really participate or engage with anyone. I found myself asking “where are the resources for people who aren’t beginners or experts?”\n\n\n\n\n“Me looking for something labeled ‘Intermediate R Resources’”\n\n\n\nSo it’s not like there aren’t any resources available at an intermediate level. DataCamp has a course (if you’re willing to pay) and there are also other resources crafted by wonderful heroes in the field that can be found by googling. But there isn’t a lot of consistency between the resources available.\nThese inconsistencies are to be expected though because there isn’t a standardized guideline that lets you know how you’re doing in the field or in R programming in general. How do aspiring data scientists/R programmers know when they’re experts? Do we ever know? Or do we have to wait until enough people deem us as “experts”? Context also matters. The set of skills one person may have may be above-and-beyond what’s needed at one company but might not cut it at another.\nI also spent way too much time thinking about when you can officially “earn” the right to call yourself a data scientist. Do you absolutely need to master machine learning and AWS before doing so? How do you know you’re not crossing into data engineering territory as those lines are kind of murky as well? Do you need to actually go out and get a degree in data science? (My opinion on that is no by the way, although for students who know this is what they want to do, it can’t hurt to get that degree if you like the program?)\nToward the end of my existential crises, I asked myself if thinking about all of this was worth it. Is it really important to label myself? Does it matter if I’m seen as an expert in the field? Does it matter if people say I’m not a data scientist until I learn more things? As long as I’m employed, probably not. What I do think is important to realize is that the climate of the data science field in general is very fluid. Especially when bringing R proficiency into the mix. I mean think about it: if you want to be a doctor, or a lawyer, or many other professions, you normally have some sort of standardized “guide” to let you know if you’re proficient in what you do. You may have options to be certified in specialized domains as well as obtain a degree to “prove” what you can do. In Data Science and R Programming, it’s a bit different.\nYou can get certified to be a tidyverse instructor. But from what I could tell, there’s no “official” or standardized certifications that exist globally for R programming. The certifications that I could find were either a part of newly designed data science programs at various universities, or certifications from Massive Open Online Courses (MOOC) on sites like Coursera or Udemy. I wonder if this is because R is an open-sourced program.\nWell, after spending a good week thinking about all of this, I came to the conclusion that I didn’t really determine too much of anything. The only thing that I WAS able to determine is this:\n\nI’m not a beginner anymore… I’m a “Not-Beginner”\n\n\nI use the term “Not-Beginner” because I don’t know if I could even call myself intermediate! I know a lot about some things, and little about others. So I am fully comfortable and proud to use this term to describe myself!\nSo how did I do it? How did I cross the imaginary boundary into “Not-Beginner” territory in data science/R programming? Keep in mind that we’re all different. Everyone learns and operates differently, but I can retrospectively tell you how I think I got here:\n\n\nEnvironment\nThis was absolutely the BIGGEST factor for me. I fell in love with R programming/Data Science in a past position while attending an AEA conference a few years back. When I returned to work, I was bursting with motivation and energy and was itching to put all of my time and effort into learning R and Data Science. The environment I was in allowed me to do this to some degree, but it wasn’t sustainable. I found myself putting in LOADS of overtime for professional development and I always felt like others in the environment didn’t have the bandwidth to really help guide me on the journey. Although it’s understandable (due to the initial learning curve you need to get over with R), that work environment left me feeling unsupported and stagnated in the process. I was forced to continue working on my skills outside of work to eventually get to the position I’m in now because it turns out it is really hard to learn new things if you’re constantly in daily Zoom meetings (I partially blame COVID on that one).\nWhile I’m the only person utilizing R in my department now, the current environment I work in is conducive to my development. It’s nurturing, flexible, and supportive which I think helps a lot.\n\n(Also not spending 70% of my time in meetings is a big help. This is a PSA and plea to abolish unnecessary meetings ✊🏾)\n\n\n\n\n\n\n\n“How I feel logging into work most days”\n\n\n\n\n\n\n\nCommunity/Online Presence\nI mentioned I’m the only one that uses R in my department and that’s totally OK. (Absolutely nothing wrong with Excel, SAS, and other data tools.) Consequently, I was prompted to create an online presence for myself to get some support. Along with the Tidy Trekker site, I also joined Twitter and made more of an effort to join data science/R programming groups on Facebook and LinkedIn. Being engaged in a community can give an extra level of support that you might not be able to get within your department. Sometimes (if data agreements allow) it can be so helpful to get outside opinions on visualizations or general assistance with programming. In particular, the #rstats and #r4ds Twitter communities are places that should be cherished and treasured.\n\n\n\n\n\n\nCome hang out on Twitter if you aren’t already there (@meghansharris)\n\n\n\n\n\n\n\nPassion/Motivation\nI recently saw a post on Twitter that stated that you should not get into the Data Science field if you’re just in it for the money, and I fully agree. I cannot tell you how many nights I accidentally stayed up coding and learning for hours. Not because I “had” to, but because I absolutely wanted to. People that are here because they are passionate about the field can understand. There were times where I had to sit and ask myself if this career choice was what I wanted purely because the process of getting through those initial phases of R programming/data science were BRUTAL. If you’re trying to get your way out of the “beginner” phases of your journey, or even if you’re just beginning, you have to be honest with yourself and make sure that it is something you truly want to do. That passion is what’s going to excel you forward in your journey.\nBack in a past position, I was overworked and sick as a result. This was around the time I was learning R. I remembered having a legitimate fever dream about coding. I would close my eyes and see the RStudio IDE staring back at me. Once that fever broke, I was thrilled to return to whatever script I was working on at the time. That’s when I knew this was for me. Now you don’t have to be at the same level of insanity that I am, clearly, but this isn’t something you should fake. If you’re not passionate about it, get out while you still can!\n\n\n\n\n\n\n“Me grinding everyday because ‘coding is a lifestyle’”\n\n\n\n\n\n\n\nPatience\nThis is the one I hate the most. I am admittedly NOT a patient person. I want to be a data science/R wizard and I wanted that status yesterday. I think of a personal example whenever this comes to mind.\nWhen I first started learning to program in R, I immediately tried to make a Shiny app. Needless to say I failed miserably and had almost quit the journey entirely. I felt so inadequate and not smart at all. I gave R a break for a week or two and realized I really wanted to learn. So I restarted the “right” way and started with the basics. I can’t tell you how long it took to get the basics down, but one day, it really did seem like things were starting to click together.\nSoon, daunting data wrangling and mining became a breeze and an enjoyable puzzle to solve. And while I am far from where I began in my journey, I still have to remember to have patience; especially as I try to navigate these murky “not-beginner” waters. Having this level of discipline can ensure that you are learning things effectively. For me personally? I just reached the point where I am finally ready to dig into the basics of machine learning and Shiny app development. I’ll be sure to document these endeavors as I trek through them.\n\nAre you a “Not-Beginner?” How did you know you weren’t a beginner anymore? If you ARE a beginner, what do you think will put you into that next category of data science “proficiency”? Feel free to leave a comment below to share or contact me directly! Respectful discourse is always welcomed!"
  },
  {
    "objectID": "post/i-participated-in-my-first-tidy-tuesday-project-recap/i-participated-in-my-first-tidy-tuesday-project-recap.html",
    "href": "post/i-participated-in-my-first-tidy-tuesday-project-recap/i-participated-in-my-first-tidy-tuesday-project-recap.html",
    "title": "I Participated in My First Tidy Tuesday! : Project Recap",
    "section": "",
    "text": "To kick off the year, I took the “leap of faith” and participated in my first TidyTuesday! A weekly community activity hosted by the R4DSCommunity on Twitter that challenges those of all skill levels and disciplines to explore a data set and make a data visualization of it. This week’s data was from the Transit Costs Project.\nThis Tidy Tuesday was challenging mainly because I was on a crunch for time. However, I did not want to pass up an opportunity to practice with Plotly (for R). In my current position, I’m tasked with building some internal dashboards to keep track of a lot of opioid data. Using Plotly can allow for interactive/dynamic graphs (instead of static, non-moving graphs.) I’ve embedded my plotly plot for this Tidy Tuesday below in this post.\nThis activity was great for practicing the obvious data cleaning and wrangling skills. I also got to practice conditional formatting (colors of the lollipops), ggplot manipulation, and lookup functions. Drawing conclusions from the data is not recommended due to the nature of the selected data sets. Tidy Tuesday really encourages developing your process as a data scientist to hone in on fundamental skills! That being said, don’t read too much into it! If you’re interested in the code used for this plotly plot, or the process used for the data, you can find it on my Github Repo here.\nFeel free to contact me directly! Respectful discourse towards efficient solutions or new insights is always welcomed!"
  },
  {
    "objectID": "post/lets-talk-about-2022/lets-talk-about-2022.html",
    "href": "post/lets-talk-about-2022/lets-talk-about-2022.html",
    "title": "Let’s Talk About 2022",
    "section": "",
    "text": "Yes, hello. It’s been a while. I have been neglecting The Tidy Trekker as of late (and to be honest, I still might this year after this overhaul is done, but we’ll see.) I had all the intentions of being super active here in 2022, but life happened. The good news is, instead of the usual negative BS, I was inundated with EXCELLENT things in 2022. I was afraid to even step into 2023 for fear that all my good luck would “run out” this year. (I’m joking btw, life is what you make it and your enjoyment of it depends on a lot of things like perspective, if you need/go to therapy, circumstances, etc.)\n\nBut enough general rambling; let’s start with some targeted rambling. So what happened in 2022?\n\nIn this post:\n\nConferences and Talks\nJob Change\nFamily and Home Changes\nSo, What About 2023?\n\n\n\nConferences and Talks\nSo this one is probably the most known. The fact that I had A LOT of talks to prepare for and do. Some of you may have found out about me through one of the various talks I did in 2022. Most of the talks were about #rtistry (generative art in the R language) but I did give more practical ones as well. The biggest one was obviously RStudio::Conf(2022). This was BY FAR the best conference experience I ever had despite being pregnant and in the middle of so many life-changing things all at once. I was literally onboarding at my new job and had jumped on a plane to D.C. less than a few days after unexpectedly closing on my first home. It was an absolute cluster of chaotic good.\nAt the conference I assisted with the Making Art From Code workshop, was a diversity scholar, and gave my own talk, “Making Data Pipelines in R.”\n\n Had to take pictures so I could know it was ✨real.✨ \n\n\nSide note. If you LOVE R and on the fence about ever going to a (Now Posit) conference, you should absolutely go if you can! In the past, if you submit a presentation, or even a lightning talk, Posit has provided travelling and boarding assistance. If you are from an underrepresented group in tech (POC, identify as a woman, LGBQTIA+ etc.) you can also apply for a diversity scholarship to gain assistance that way as well. Just like how Twitter used to be awesome (before the exodus I mean), imagine having that in real life for a few days. It was awesome. And I mean, come on, who doesn’t like stickers???\n\n\nSo…Many…STICKERS!!!!\n\n\n\nJob Change\nOne of the cool things about going to the conference was that I was able to meet some of my co-workers in real life! I recently changed jobs in 2022. I now work at the Prostate Cancer Clinical Trials Consortium (PCCTC) within Memorial Sloan Kettering Cancer Center. This has been a wonderful change so far as it had allowed me to really stop working so much. I was able to go to from having three jobs to one full time one (and a Per Diem contract one if needed from time to time.)\nI am obviously on parental leave now, but when I get back, I’m excited to get to work. Some of my work is going to focus on streamlining internal stuff for reporting on prostate cancer studies and I’m SUPER hyped about it. I’ve been using R for about four, going on five years now, and I think I’m finally starting to hone in on my “niche.” Besides making art with R, I get so much satisfaction out of automating things. Building (hopefully) seamless pipelines that makes work easier for people. I realized I’d rather spend months building up pipelines than doing things like analyses, external client-facing, and the like. In other words, my current role as a data scientist in this capacity lines up perfectly with my pragmatic interests, and honestly, I’m super lucky there.\n\n\nFamily and Home Changes\nWorking at the PCCTC also went perfectly with home life. For those of you that missed it, I was very open about my infertility struggles after I got pregnant. But as life would have it, after trying aggressively for three years, I got pregnant right before I found out about the open position at the PCCTC, and I honestly almost didn’t apply for it because it felt wrong to do so knowing that I would have to leave for parental leave.\nLooking back on it, switching jobs was the absolute best thing I could have done. I was able to support myself financially, close on my first home, buy a car that wouldn’t break down me, and support others in my family when they needed it the most. Even now, I’ve been able to spend time getting to know my son, and just being grateful that all the years of feeling like I was working myself to death eventually paid off (yes, even during late nights while I was on the verge of tears trying to console a crying baby 😅)\nI have a lot to be grateful for. I feel like I have changed so much as a person and that I am really coming into the best version of myself that I can be. Whether we’re talking professionally or personally.\n\n\nSo, What About 2023?\nSo… what’s next? Honestly, this year I’d like to “lay low.” I want to actively just stay home, study up on things, be a mother, and just work. That’s all I want. Professionally speaking, this is what I’ve got on my to-do list for 2023:\n\nGet projects done/foundations started at work (This has to be done as I have to work lol)\nOpen up my online art store (This doesn’t have to be done, but I probably should since I already made the store and everything.)\nRe-launch the new version of my website (If you’re reading this, that means I did this! Go Me!!!)\nContribute to the textbook “Rtistry: Methods for Visual Generative Art Using R” along with Antonio Paez, Antonio Sanchez, and Jacquie Tran. (This has to be done as a contract was signed)\n\nThat’s it. If I have energy for a blog post here or there, or if my job wants to send me to Chicago for Posit Conf this year (Travis pls.) Then I’ll do that. But the goal is to work passively this year (if that makes any sense at all.)"
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html",
    "title": "Making My First Shiny App",
    "section": "",
    "text": "This year was the first time that I ever competed in a RStudio Shiny competition by submitting my first ever Shiny app, the TarotreadR. For those of you that read one of my previous blog posts “How I Became a ‘Not-Beginner’ in R”, you’ll know that I tried to make Shiny apps first before doing anything else in R and RStudio. This was before I tried learning basic programming in, or even learning how to load data into, the RStudio environment. I can credit Shiny with being the reason why I almost gave up on learning R and almost did not make a transition into the field of data science.\nNow fast forward about 2-3 years to present day; I can comfortably say that I still know so little about Shiny. In March of this year, RStudio’s 3rd Annual Shiny Competition was first announced and, for some reason, I felt compelled to face my fears and enter the competition. The thought of competing was honestly terrifying and daunting because I had never worked in Shiny— well, besides my first failed attempt over 2 years ago. Just like in the past with my machine learning experiences, every time I tried to get started with reading materials about creating my own app in Shiny, I would just freeze up mentally. Everything always seemed overwhelming, and I could never understand how people could create such amazing and intricate apps just using Shiny. Despite all of this, I somehow managed to pull together a working app. The process involved a lot of resources, a lot of time, some tears, and some sleepless nights. Although I got no honorable mentions or placements in the competition, I have to say that it is something I am extremely proud of. Plus, I also got my first hex stickers for free so I can’t complain at all!\nA few months ago, I could never have imagined that I would be able to create a working app that I made by myself in Shiny. The TarotreadR isn’t the best app and might not be the most creative, but it is something that is a huge accomplishment for me. Especially so because it was made in about three months with no prior Shiny experience. I wanted to take the time to talk about this process of creating the TarotreadR and what I learned from the experience, so let’s dive in!"
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#the-idea",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#the-idea",
    "title": "Making My First Shiny App",
    "section": "The Idea",
    "text": "The Idea\n\n\n\nThe Modern Witch Tarot Deck by Lisa Sterle. Image credit: Lauren Panepinto | Muddycolors.com\n\n\n\nMy TarotreadR idea was thought up before the Shiny competition this year. Recently, I’ve personally become interested in tarot cards (and was gifted The Modern Witch Tarot Deck by Lisa Sterle.) This is not because I was planning my career transition into tarot reading, but because I grew fond of the different art styles that different decks can have. I find it so interesting to look at how artists’ interpretations of these decks vary. I must also admit that I do like pulling cards for myself from time to time. I find that some people get surprised when I tell them that. I’m viewed as this highly analytical person that works in data science and “real things.” For me, I liken reading cards to using a fidget spinner in some sense.\n\nAs a mental health advocate, I’ve always been open about my mental health struggles and I must say that sometimes pulling tarot cards prompts me to use the Barnum Effect to my advantage by creating some distance from my anxious thoughts. This gives me the space to acknowledge what I already know within me. Being that I started doing this more frequently, I wanted an app that could spell out full interpretations of groups of tarot cards based on what was pulled. Interpreting tarot cards can get messy and confusing if you start doing more than one, and if you allow them to be reversed. It can get complicated because a reversed tarot card can have subtle but key differences in interpretations when compared to upright cards. So, given that I was already interested in this and wanted to do it, the idea of a simpler app, the TarotreadR, was born.\n\nInstead of making fully tailored interpretations, the app would randomly pull cards for the user and give them keywords for each card based on if the card was upright or reversed. To get started with this, I wanted to do the tasks that I thought would take the longest: making the card images and creating tarot card data."
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#making-the-card-art-and-data",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#making-the-card-art-and-data",
    "title": "Making My First Shiny App",
    "section": "Making the Card Art and Data",
    "text": "Making the Card Art and Data\n\n\n\n\nOne of my favorite cards that I made. This gif shows all the pieces I had to individually find to create the visual for this card.\n\n\n\nThe process of making the card art was a bit daunting but something I really wanted to do. The original tarot deck consists of 78 cards in total. Making the card art did take about 2 months over the course of a few sessions. It was the perfect time to get some use out of my Pro Canva account. I created each image by “collaging” various icons, graphics, and shapes together. This was a tedious process as it required thinking outside of the box to get the shapes and figures I wanted. The hardest part was trying to keep the figures (human silhouettes) consistent. Let’s just say I like some cards more than others.\n\n\n\n\nOne of my best cards and one of my worst. Note the inconsistency in the style and design of the human silhouettes.\n\n\n\nWhile I was making the card art, I was also creating a dataset to go with it. For an app like this, the dataset was straightforward. I would just need three variables: The card’s name, the associated keywords (card meanings), and the position (upright or reversed/upside down) of the card. I used a combination of published meanings from biddytarot.com and labyrinthos.co to fill in the keywords of each card. This process also took about a few months as I only filled in the data once I completed the art for the card.\n\n\n\n\nThe finished tarot card dataset in the TarotreadR Github Repository\n\n\n\nCreating the dataset was an iterative process. While I was working on the cards and data, I did start to think about the app structure. Trial and error eventually lead to me figuring out that the text in the “keywords” variable could be in plaintext HTML. This required me fixing the data to follow plaintext HTML and to then introduce line breaks in the UI of the app when keywords were displayed.\n\n\n\n\nThe Three of Cups card displayed in the TarotreadR app and a snip of the dataset. Plaintext HTML was used to format the keywords.\n\n\n\nAfter I had a good foundation going with the cards and dataset, I had to finally face my fears and start trying to build the app in Shiny. I thought the best way to do this would be to try to wrap my head around the Shiny environment. I would be lying if I said I wasn’t terrified."
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#understanding-the-shiny-environment-and-reactivity",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#understanding-the-shiny-environment-and-reactivity",
    "title": "Making My First Shiny App",
    "section": "Understanding the Shiny Environment and Reactivity",
    "text": "Understanding the Shiny Environment and Reactivity\n\n\n\nMe opening a new default Shiny app script in R.\n\n\n\nSpoiler Alert: this part was actually not that bad! The worst is designing your app to make it look pretty and I’ll get to that nightmare shortly. If you’ve read one of my previous R walkthroughs, Making Dull Dashboards, you may recall me talking about the concept of reactivity when creating flexdashboards. The concepts I learned using the Crosstalk and SummaryWidgets packages to make flexdashboards prepared me to comprehend how the Shiny environment works. While I am still a Shiny noob in my mind, it’s definitely not as scary when you have a basic understanding of how the two major components of a Shiny app work together.\n\n\n\n\nA basic thought process that helped me understand the Shiny environment.\n\n\n\nI think of a Shiny app as if it were a car made of two main components: the Server and the UI (User Interface). I think of the Server as everything under the hood of the car: the engine, fuel tank, electrical system, etc. I think of the UI as the frame of the car, the seats, the steering wheel, and anything else I would need to comfortably control the car.\nI think of it this way: there’s a bare minimum that’s required for a car to turn on and operate. This is like having functions in your Server that, at its bare minimum, will allow the app to operate regardless of efficiency or the warnings that we might ignore in the console while our app is deploying. At its bare minimum, the UI should similarly be able to give the user basic functionality. In our car comparison this could be something as simple as making sure the car has a working ignition and a steering wheel. In the same sense, while our cars can operate with these bare minimums, we’d prefer to not see a check engine light, we’d prefer seat belts, and maybe some nice leather seats to sit on.\nOnce I started thinking about Shiny apps this way, I realized the importance of the interaction between the UI and the Server. I’m very excited to build from this knowledge to make better apps in the future, but for now, this basic understanding was all I needed to make the TarotreadR happen."
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#designing-the-ui",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#designing-the-ui",
    "title": "Making My First Shiny App",
    "section": "Designing the UI",
    "text": "Designing the UI\n\n\n\nA piece of the UI code used to make the TarotreadR. This code creates a one card draw.\n\n\n\nCreating the UI was also an iterative process. It seemed I was constantly going back and forth between the UI and Server through trial and error. I expected this as I was essentially teaching myself as I progressed. One of the biggest challenges was learning that it’s REALLY helpful to have some knowledge of HTML, CSS, and even JavaScript when designing a Shiny app’s UI. While it’s not required to make an app functional, it is required to make an app pretty. I had a vision of what I wanted the app to look like, and to get that vision, I definitely had to spend some nights refreshing my HTML skills and learning new skills in CSS and JavaScript. These backgrounds were needed to figure out small things like not having cards appear until a user has clicked on an action button and having the cards animate onto the screen when a user “pulled” them.\nAnother challenge was figuring out spacing and sizing. It required thinking about the app and its components in a modular fashion. Boxes within boxes, if you will. Again, trial and error was my best friend here. The code you see above is a “Frankenstein” masterpiece that came to be after many iterations and failed deployments."
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#setting-up-the-server",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#setting-up-the-server",
    "title": "Making My First Shiny App",
    "section": "Setting Up the Server",
    "text": "Setting Up the Server\nBesides trying to teach myself CSS and minimal JavaScript in less than a month, one of the most difficult things about building the TarotreadR was figuring out the Server component. I needed to figure out how to randomize cards being pulled, but I also needed to get images to show up in the UI based on those randomizations. Things get even more difficult once you add in the reversed card positions.\nMy original thought was to only make the deck of cards one time and then figure out how to get the images to display in a reversed position programmatically. In my darkest hour, I gave up trying to figure out the CSS to make this happen. Every time I thought I had the answer, the image would somehow be off and not displaying correctly. In that moment, I shed a tear, closed RStudio, and opened up Canva and proceeded to make “reversed” images for each of the cards. In retrospect, I really wish that I would have stuck it out as it might have improved the TarotreadR’s performance (instead of loading 78 card images, it was now loading 156 card images for upright and reversed positions).\nHaving separate images did end up working for me with the help of this post by Jeroen Ooms. Once I got over the biggest hurdle of getting the images to display properly, I was able to add things like toggling (so that cards are only shown after a button is clicked) and sounds that play with the card animations.\n\n\n\n\nA snip of the animations and interactions coded for the TarotreadR. Most of these functions came from packages like ShinyAnimate."
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#putting-it-all-together",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#putting-it-all-together",
    "title": "Making My First Shiny App",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nFinally, after about three months, all of the work paid off! I finally finished all of the cards, completed all of the data entry, got the UI and Server to talk to each other, and successfully deployed the TarotreadR to Shinyapps.io. If you haven’t seen it action and would like to, you can here.\nIf you’d like to see the full code and files used to make all of this, you can find it in my Github Repo here.\n\n\n\n\nThe completed TarotreadR app. My first Shiny app!"
  },
  {
    "objectID": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#lessons-learned",
    "href": "post/making-my-first-shiny-app/making-my-first-shiny-app.html#lessons-learned",
    "title": "Making My First Shiny App",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nAfter this whole experience, my professional and personal life got extremely hectic, and I didn’t get a chance to really talk about this publicly (which is what prompted this post). There are a few lessons I’ve learned that I want to highlight:\n\nCSS, HTML, and JavaScript are crucial to making attractive UIs (in my opinion).\nSometimes it’s OK if your code is not the most efficient. You can still get your app to work – but it is in good practice to try and revisit the code to improve it if possible.\nFlexdashboards, Crosstalk, and SummaryWidgets are absolutely the reason why I was able to do all of this in three months. Learning these packages can ease you into understanding how app reactivity works.\nI have even more respect for web devs and those that make Shiny apps for a living (I already had a lot of respect for you guys, now it’s just astronomical).\nShiny isn’t all that scary once you slow down and take the time to understand the environment.\n\n\nHave you ever made Shiny apps before? Do you have “Shinyphobia” like I did? Are you screaming at the inefficiency of my program? (I know, I’m sorry!) Feel free to contact me directly! Respectful discourse towards efficient solutions or new insights is always welcomed!"
  },
  {
    "objectID": "post/making-waves/making-waves.html",
    "href": "post/making-waves/making-waves.html",
    "title": "Making Waves in ggplot: An Rtistry Tutorial",
    "section": "",
    "text": "The Original “Wave Patch” Piece Made in R\nA little while ago, I made an Rtistry piece that I named “Wave Patch.” (Shown above). There seemed to be some interest in providing a tutorial on making waves like I did in Wave Patch. I’ll use this post to discuss the basic concepts behind making one set of waves that will hopefully set the reader up to take that knowledge, expand upon it, and make their own Rtistry!\nWhile things will always be explained when appropriate, this tutorial assumes that you have experience working in ggplot in the tidyverse, working with ggplot aesthetics, and creating functions.\nDon’t want to read and just want the code? The full code for this tutorial is on my GitHub Repo here."
  },
  {
    "objectID": "post/making-waves/making-waves.html#lets-start-with-a-trig-refresher",
    "href": "post/making-waves/making-waves.html#lets-start-with-a-trig-refresher",
    "title": "Making Waves in ggplot: An Rtistry Tutorial",
    "section": "Let’s Start with a Trig Refresher!",
    "text": "Let’s Start with a Trig Refresher!\nSo, confession time. I have never taken a trigonometry class in my life! The closest I got was taking geometry in high school and I got a C. BUT, I was still able to make this piece, so no matter your math background, you can too! These waves are made with sine waves. You might be familiar with them already. We can code for basic sine waves in ggplot by setting up our data appropriately:\n\n# Library Load-In====\nlibrary(tidyverse) #For everything data#\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   1.0.1 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.3.0      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n#Let's Start with a Trig Refresher#========================#\ntheta <- seq(from = 0,\n             to = 2*pi, \n             length.out = 100)\n\nsine <- tibble(x = theta,\n               y = sin(theta),\n               label = 1:length(theta))\n\n\n… and plotting it like this:\n\n# A basic sine curve======\nsine %>%\n  ggplot(aes(x=x,y=y))+\n  geom_line(color= \"red\", size = 3)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nAll we are doing with this code is setting up a sequence of points in our theta variable. Using the seq() function. We’re telling R “Create 100 numbers between and including the values 0 to 2pi (6.283185). When we go to plot this data, we put theta on one axis (in this case, the x variable because I want my wave to be drawn horizontally on the plane), and then place the sine values of theta on the other axis. Note that these values don’t have to be 0 and 2pi. In the next section, we will switch this up. But let’s just take one more look at this wave with some labels attached:\n\n# A basic sine curve with more detail pointed out======\n\nsine %>%\n  ggplot(aes(x=x,y=y, label = label))+\n  geom_vline(xintercept = 0, size = 2)+\n  geom_vline(xintercept = 2*pi, size = 2)+\n  geom_line(color= \"red\", size = 3)+\n  geom_point(color = \"blue\")+\n  ggrepel::geom_text_repel(max.overlaps = 20, size = 3)+\n  geom_text(aes(x = 0,y = -1),\n           label =paste(sprintf('\\u2190'),\"theta's '0'\"),\n           nudge_x = .5,\n           size = 3)+\n  geom_text(aes(x = 2*pi,y = 1),\n           label = paste(\"theta's '2*pi'\",sprintf('\\u2192')),\n           nudge_x = -.5,\n           size = 3)\n\n\n\n\n\n\n\n\n\nIf needed, take some time to review the more detailed code/plot of the labeled sine wave. Understanding how data is selected to create the visuals on the plot will set you up for understanding how you can manipulate these visuals in the future."
  },
  {
    "objectID": "post/orderingmonths/ordering-months.html",
    "href": "post/orderingmonths/ordering-months.html",
    "title": "Ordering Months",
    "section": "",
    "text": "The Problems: 1) You need to order your “Month” categories chronologically on a ggplot but they keep plotting alphabetically.\n2) You don’t have data for all 12 months of the year, but want to have every month of the year plotted on your graph.\nThe First Example:\nHere we have simple aggregated data for total new patients enrolled in a clinic during the year. The dataset might look like this:\n\n\n\n\nThe “Patients” Example Data\n\n\n\nFor those of you who are just starting out in ggplot and have managed to plot your first graphs, you may have realized that ggplot doesn’t always display your categorical data as you see it in your view pane. Often times, ggplot will plot categorical variables in alphabetical order. This happens because the categorical variable you are trying to graph (“Month”) is not set as an ordered factor. In R, factors are categorical variables that have values assigned to them. To learn a bit more about factors, I highly recommend referring to chapter 15 (Factors) in Wickham and Grolemund’s book R For Data Science.\nIf you’d like to follow along within the R project files you can download them from the Tidy Trekker Github Repository. Be sure to fully extract the ZIP. folders for proper access. You can also download the data set used and load it into your own session for practice by clicking the GitHub button below:\n\n\n\nDirect download link for this post’s example data\n\n\n\n\n\nFor this example, we’ll use the readr, dplyr, tidyr, and ggplot2 tidyverse packages to load in our data, do some wrangling, and get it visualized on to a graph. We’ll load in our libraries and our first dataset named Patients.\n\n\n# Loading in the appropriate libraries===\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n# Loading the data into the environment===\nPatients <- read_csv(\"data/Patients.csv\")\n\nRows: 5 Columns: 2\n\n\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Month\ndbl (1): New Patients\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n Pay attention to the file path in the read_csv function. If you copy and paste that command and are not using the project files within the Github link above, you will get an error!\n\n\n\n\n\nSo we have our data in, and we decide we want to make a simple bar chart of these counts. We’ll use a basic ggplot framework to do this. We’ll call the resulting plot Patientgraph. If you’re rusty or just learning ggplot, I highly recommend downloading the ggplot cheat sheet to get up to speed!\n\n\n# Making a ggplot with the data \"as-is\"====\nPatientgraph <- ggplot(Patients, aes(x = Month, y = `New Patients`)) +\n  geom_bar(stat = \"identity\", fill = \"#693ead\")\n  \n# Let's view the graph===\nPatientgraph\n\n\n\n\n\n\n\n\n\nWe can see that we’ve got our data onto the graph, but the Month categories are displayed alphabetically. This may be fine in some cases, but usually we expect to see months listed chronologically (January - December).\nWe can already see from the graph and dataset that we have 5 distinct months to work with. We can confirm this by asking R to show us the unique values for the Month variable. We should also take the time to identify the Month variable’s class. (Data Type)\n\n\n# Viewing the unique values in the \"Month\" variable===\nunique(Patients$Month)\n\n[1] \"February\"  \"May\"       \"August\"    \"September\" \"December\" \n\n# Identifying the class of the \"Month\" variable===\nclass(Patients$Month)\n\n[1] \"character\"\n\n\n\n\n\n\nAs I mentioned earlier, in order to adjust these categories on a ggplot, we need to change the variable type, or it’s class, into a factor. Let’s make this change and save the results into a new dataset called “Patients2.”\n\n\nPatients2 <- Patients %>%\n  mutate(Month = factor(Month, levels = c(\"February\",\"May\",\"August\",\"September\",\"December\")))\n\n Using the pipe operator %>% with the dplyr verb mutate, we can alter the existing Month variable into a factor using base R’s “factor” function. In this function, you simply tell R which variable you want to convert and then pass a “levels” argument. This argument manually dictates the order in which categories will be seen on the ggplot. Note that because we are using the pipe operator, we do not need to do a direct subset to this variable in the factor function. ex: (Patients$Month)\n\n Although cumbersome, manually setting your factor levels may be required from time to time. Make sure capitalization, spelling, and punctuation match what’s in your data set exactly. Even having whitespace in your data can cause issues. You can tackle whitespace issues with Stringr’s str_trim function.\n\n\nNow let’s replot the graph with the Patients2 data.\n\n\n# Plotting Patients2===\nPatientgraph2 <- ggplot(Patients2, aes(x = Month, y = `New Patients`))+ \n  geom_bar(stat = \"identity\", fill= \"#693ead\")\n\n#Viewing Patients2 graph===\nPatientgraph2\n\n\n\n\n\n\n\n\n\nNow this is more like it! Now I did say that this method was cumbersome. Thankfully, there are ways to cut some of this work down. We’ll do this by using some constants in R.\n\n\n\nIf you don’t know about constants, you can read about them here! They are basically preset values that come in base R. We can use the month.name constant in this particular example. Let’s look at it first though to get familiar.\n\n\n# Printing out the month.name constant to take a look at it===\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\nWe can see all the months are here in this constant ready for us to use! So how do we apply this to help us out with our coding? By calling on the month.name constant for our levels argument when factoring! Recall that we have five months in our dataset. In order to use the “month.names” constant, it has to only contain those five months (Remember that the levels have to match exactly.) We do this by subsetting out only the months we need. We’ll do this and store the results into a dataset called “Patients3” with the following code:\n\n\n# Refactoring the \"easier\" way===\n    Patients3 <- Patients %>%\n        mutate(Month = factor(Month, levels = month.name[month.name %in% unique(Month)]))\n\n\nYou may see that our code looks similar to the last factoring we did. The only thing that has changed is the addition of what’s passed into the levels argument:\n\n\nmonth.name[month.name %in% unique(Month)]\n\n\nThe code above tells R to “look” within the “month.name” constant and only return the values that are also uniquely in (%in%) our “Month” variable. If needed, you can learn more about R’s operators like %in% here. If we plot Patients3, we can see the result is the same as the original refactored graph we made.\n\n\nPatients3\n\n# A tibble: 5 x 2\n  Month     `New Patients`\n  <fct>              <dbl>\n1 February              21\n2 May                   16\n3 August                33\n4 September             40\n5 December              11\n\n\n\nThe only difference here is not having to do the cumbersome coding of spelling out our months in our factor function.\n\n\n\nSo what if we have a dataset with all the months accounted for in the year? We can do less work! Let’s load in our Patients_complete dataset to take a look.\n\n\n# Loading in the \"complete\" Patients dataset\nPatients_complete <- read_csv(\"data/Patients_complete.csv\")\n\nRows: 12 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Month\ndbl (1): New Patients\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n Pay attention to the file path in the read_csv function. If you copy and paste that command and are not using the project files within the Github link above, you will get an error!\n\n\n\n\n\n\nThe “Patients Complete” Example Data\n\n\n\n So you might’ve guessed that this dataset would also return a graph in which the months are ordered alphabetically. We’ll make a quick one and store it in Patientgraph4.\n\n\n# Confirming assumptions with the \"Patients_complete\" dataset===\n  Patientgraph4 <- ggplot(Patients_complete, aes(x = Month, y = `New Patients`)) +\n  geom_bar(stat = \"identity\", fill= \"#693ead\")\n  \n# Viewing it==\nPatientgraph4\n\n\n\n\n\n\n\n\n\nThis time, because we have all of the months in the dataset, the code for refactoring can be simplified. We’ll store it in a dataset called Patients4 then graph it in a ggplot called Patientgraph5.\n\n\n# Easier refactoring when all months are present ===\nPatients4 <- Patients_complete %>%\n  mutate(Month = factor(Month, levels = month.name))\n  \n# Creating the plot ===\nPatientgraph5 <- ggplot(Patients4, aes(x = Month, y = `New Patients`))+\n  geom_bar(stat = \"identity\", fill= \"#693ead\")\n\n# Viewing it ===\nPatientgraph5\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s go back to our original Patients dataset. What do you do if you don’t have all the months of the year, but you want to plot all of the months? To do this, we need to create “placeholder” data points within our dataset. We do this by adding the months we are missing to our dataset and assigning them a numeric value of 0. We’ll do this and store it into a final dataset called Patients_modified. Be sure to remember to refactor this as well!\n\n\n # Adding missing months to our \"Patients\" data set===\n Patients_modified <- left_join(tibble(\"Month\" = month.name),Patients, by = \"Month\") \n \n #Replacing coerced NAs from the previous code to a numeric 0 then refactoring our new dataset===\nPatients_modified <- Patients_modified %>%\n  mutate(`New Patients` = ifelse(is.na(`New Patients`),0,`New Patients`)) %>%\n  mutate(Month = factor(Month, levels = month.name))\n\n\nEssentially what we’ve done is transformed our vector of month.name constants into a data frame, or tibble. Doing this allows us to complete a left join and merge all of the “Month” names in the constant into our dataset. The first mutate function uses the ifelse function to replace all the NAs of this merge with numeric values of 0. The second mutate function is just the refactoring method that was previously introduced. Let’s create the graph, call it Patientgraph6, and view it.\n\n\n# Creating the final plot===\nPatientgraph6 <- ggplot(Patients_modified, aes(x = Month, y = `New Patients`)) +\n  geom_bar(stat = \"identity\", fill= \"#693ead\") \n\n#And view it===\nPatientgraph6\n\n\n\n\n\n\n\n\n\nAnd there we are! All of our data is plotted and we can see all of the months in the year! You’ll notice those pesky month labels may be colliding with each other. This may vary based on the machine/browser window you’re working with. If you need to fix cases like these, I’d recommend the str_wrap() function from the stringr package.\nHave any thoughts or suggestions? Know of a better solution or way to make this process more efficient? Feel free to leave a comment below to share or contact me directly! Respectful discourse towards efficient solutions or new insights is always welcomed!"
  },
  {
    "objectID": "post/setting-intentions/setting-intentions.html",
    "href": "post/setting-intentions/setting-intentions.html",
    "title": "Setting Intentions",
    "section": "",
    "text": "No doubt, 2020 has been extremely difficult for everyone. (Who would have known it would be so draining to be productive and just live during a global pandemic?) Many people were forced to adapt to a “new normal” (don’t worry, I won’t use the “U” word we’re all tired of hearing.) Many of us in the U.S who were fortunate to remain employed had to adapt to working from home (WFH). This personally has not bothered me as I’m an advocate for WFH even during non-pandemic times due to the benefits it brings I’ve been fortunate enough to continue working during the pandemic and actually switch jobs to take baby steps into my data science career.\nGiven that the theme of 2020 seemed to be “instability”, I thought it would be wonderful to have just one thing I could consider “stable.” My own little pocket on the web. I hope to document my progression into data science and finally make some contributions to the field. I have my reservations about this. Candidly, I consider myself a “baby data scientist.” My path into data science was by no means “traditional.” I will be the first to say that my weakness is not having a formal computer science or statistics degree. Back in my day, (*shudder* I sound old.) data science wasn’t as well known or prominent as it is today. If you were majoring in computer science, it was because you were going to do hardcore computer programming. Retrospectively, a career in data science/coding made sense for me even in my adolescence. (I proudly spent hours learning CSS and HTML to have the best Myspace page. I have also always loved applied statistics; hindsight truly is 2020. Yes, that is a pun. No, I’m not sorry.)\nThe goal of this site is to help myself and anyone else while I progress into this field. I hope to offer a different perspective to solutions in data science based on needs/functionality. I am by no means an expert and I want to make that OK. I definitely am affected by imposter syndrome taking this route into data science but I’d like to serve as an example for others on a similar path. (You can learn more about imposter syndrome through a data scientist’s perspective by watching fellow data scientist, Ken Jee’s , video on it here.) I want to encourage others to continue to push through and learn about this field despite the challenges. On a personal note, I want to learn all that I can so that I can advocate and push for more data science efforts in health research and mental health data analytics.\nSo, moving forward, I’ll be using the Exploration Corner to share various categories of posts that include:\n\nGeneral/Data Musings: Informal posts like this one that you’re reading right now that are general or my thoughts/opinions on data-related things that I find.\nProject Recaps: These will be posts that briefly describe projects I have completed. Project recaps will differ from full walkthroughs as they will not be a step-by-step guide. It will generally explain a goal, data sources, a quick list of processes/packages used, and a link to the actual project and source code on Github.\nR Explorations: These posts will act as a playground for me personally. I’m reserving this space as a way to leave notes for myself that might help others. This can include deeper dives into packages and functions or even cheat sheets.\nR Walkthroughs: These posts will be targeted step-by-step guides on how I’ve done random things in R. I use R daily for my work and find myself doing different things daily. These will also include links to example data sets and project files on Github when applicable.\nAs I trek through my journey, I do plan to learn python and dive into D3.js. When I get to that point, I’ll add blog categories accordingly.\nWhere are you on your data science journey? Do you have any suggestions for different blog posts? Feel free to leave a comment below to share your thoughts or contact me directly! Respectful discourse is always welcomed!"
  },
  {
    "objectID": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html",
    "href": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html",
    "title": "Tallying “Checkbox” Survey Responses: R Walkthrough",
    "section": "",
    "text": "The Problem: You need to tally (add) checkbox survey responses, but they are combined and comma-separated together in one string.\nThe Fix: You need to count the occurrences of each response within each observation’s string. You can do this in a few lines with tidyr and dplyr, or in a more involved way while practicing string manipulation with regex/grepl and stringr."
  },
  {
    "objectID": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html#the-example",
    "href": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html#the-example",
    "title": "Tallying “Checkbox” Survey Responses: R Walkthrough",
    "section": "The Example:",
    "text": "The Example:\nLet’s say we have a simple survey question that asks participants to select colors that they like. We allow them to “Check all that apply” and give them eight choices to choose from. It might look like this:\n\n\n\n\n\n“A snapshot of example checkbox survey questions”\n\n\n\n\n\nCheckbox question-types allow participants to select multiple choices. This can be useful for building strong analyses. However, we may find that the “checkbox” question-type produces undesirable data that will have to be dealt with. Let’s explore this by setting up our R session.\nIf you’d like to follow along within the R project files you can download them from the Tidy Trekker Github Repository. Be sure to fully extract the ZIP. folders for proper access. You can also download the data set used and load it into your own session for practice.\n\n\n\nDirect download link for this post’s example data"
  },
  {
    "objectID": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html#using-separate_rows-from-tidyr-and-count-from-dplyr",
    "href": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html#using-separate_rows-from-tidyr-and-count-from-dplyr",
    "title": "Tallying “Checkbox” Survey Responses: R Walkthrough",
    "section": "Using “separate_rows” from Tidyr and “count” from Dplyr:",
    "text": "Using “separate_rows” from Tidyr and “count” from Dplyr:\nThe simplest and fastest way to tally up these results is to use tidyr and dplyr together. We start by loading in the packages we need: readr, dplyr, and tidyr. These allow us to get our data into the environment and prepare it for cleaning and manipulation. We’ll name the resulting data set “Colors.”\n\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\nColors <- read_csv(\"Data/Colors.csv\", col_types = cols(`What colors do you like?` = col_character()))\n\n\n\n Pay attention to the file path in the “read_csv” function. If you copy and paste that command and are not using the project files within the Github link above, you will get an error!\n\n\nWe can see our data set in the data viewer. Notice that each observation has a combined, comma-separated string for each response:\n\n\n\n\n\n\n“A snapshot of example checkbox survey question loaded into a dataframe”\n\n\n\n\n\nWe need to tell R to separate the values in the What colors do you like? column into individual rows and then count and tally those occurrences. Let’s do this and store it in a data set called TidyrColors. The code for this can be written in one chunk with the following:\n\n\nTidyrColors <- Colors %>%\n  separate_rows(`What colors do you like?`, sep = \", \") %>%\n  count(`What colors do you like?`, sort = TRUE, name = \"Tally\")\n\nThe separate_rows() function from tidyr separates all the values in the column while using “,” as the characters to separate on. Note that we have a whitespace after the comma in the sep= option. This is because whitespaces are present in the original data. If you had options that were only separated by commas and no spaces, you would just need a comma there.\nNext, we used the count() function from dplyr to count the occurrences of our colors. Setting the sort= option to TRUE will arrange the observations in descending order in the dataset. The name= option will allow us to set the column name that will hold the count of the values. In this case, we’ve set it to Tally.\n\n\n\n\n\n\n“A snapshot of example checkbox survey results separated and tallied together”\n\n\n\n\n\nAnd that’s it! Pretty simple and straightforward. Continue on for more involved ways to do this that will give you some string/regex practice if interested!"
  },
  {
    "objectID": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html#using-grepl-or-stringr-dplyr-and-tidyr",
    "href": "post/tallying-checkbox-survey-responses-r-walkthrough/tallying-checkbox-survey-responses-r-walkthrough.html#using-grepl-or-stringr-dplyr-and-tidyr",
    "title": "Tallying “Checkbox” Survey Responses: R Walkthrough",
    "section": "Using Grepl (or Stringr), Dplyr, and Tidyr:",
    "text": "Using Grepl (or Stringr), Dplyr, and Tidyr:\nThere are other ways we can do this that are a bit more involved, but can provide some grepl/regex and string manipulation practice if desired. If you choose to use Stringr instead of grepl, you can load in the stringr package as well.\n\n\n# If using the stringr package===\nlibrary(stringr)\n\n\nIn order to get a tally of the individual responses, we need to break up these strings and create variables for each color. We do this by creating a new column for each of our color options available for our survey question. Let’s create a reference vector of all of our color options to help us. We’ll name it Colorref:\n\n\nColorref <- c(\"Red\",\"Blue\",\"Green\",\"Yellow\",\"Black\",\"Orange\",\n\"Brown\",\"Pink\")\n\n\n\n Pay attention to the strings you enter into your reference vector. These strings need to match your survey responses/options PERFECTLY. Be mindful of any discrepancies in capitalization, spelling, spaces, or punctuations.\n\n\nUsing the reference vector we just created, we can now create an empty data frame that we can populate with our color tallies. We’ll call it Colorsnew. We can use base functions in R to do this. To ensure the tallies are properly recorded in the new data frame, we’ll set the number of columns to match the number of options (Colors) we have and the number of rows to match the number of observations (or rows) we have in the original data set. Afterwards, we replace the column names with the name of each color choice. This can be done with the names function.\n\n\nColorsnew <- as.data.frame(matrix(ncol = length(Colorref), nrow = nrow(Colors)))\nnames(Colorsnew) <- Colorref\n\n\nThe next step is to simultaneously populate all the columns in the data set with an accurate tally for each response. We can do this with the ifelse and grepl functions within one single “for-loop.” This will populate our currently empty Colorsnew data set so we can easily see our changes. The for-loop function will run iterations along the length of the Colorref vector and use the vector’s values to fill in the column names of our data. At the same time we’ll be getting getting a count of how often each color is present in the original Colors dataset with the grepl function. Detailed information about for-loops can be found in the Iteration chapter in the free textbook R for Data Science (written by Hadley Wickham and Garrett Grolemund) :\n\n\nfor (i in seq_along(Colorref)){\n  Colorsnew[i] = ifelse(grepl(Colorref[i],Colors$`What colors do you like?`),1,0)}\n\n\nTo reiterate, this code uses the i variable as a placeholder to rename the columns within the Colorsnew data set with the strings found in Colorref (All of our color options.) For each column, we are using an ifelse() test to scan the original strings in the What colors do you like column from the Colors data set (Colors$What colors do you like?). Simultaneously, we check for matches using the grepl function. If you aren’t a fan of using grepl, you can also opt for Stringr’s str_detect() function that performs the same action.\n\n\nfor (i in seq_along(Colorref)){\n  Colorsnew[i] = ifelse(str_detect(Colors$`What colors do you like?`,Colorref[i]),1,0)}\n\nWhether you use grepl or stringr, the result will be the same. Each match will produce a “1” while non-matches produce a “0.” The result is a matrix of observations for all of our color choices:\n\n\n\n\n\n\n“A snapshot of example checkbox survey results separated”\n\n\n\n\n\nOur last step will be to transpose this wide data into a long data format for easier graphing and analyses. (You can check out a tutorial on wide and long data in R from DataCamp here.) We do this using the pivot_longer, group_by, and summarise functions from the dplyr and tidyr packages. In our pivot_longer function, we tell R we want to transpose all of the columns in the “Colorsnew” dataset with dplyr’s everything() option. We tell R that we want all of our current column names to be put into one variable called Color and that we want the values in these columns to be put into its own column called Tally. Finally, we want to group the data set observations by the “Color” column and add all of the tallies up so we get a sum for each value within the Tally column. This will produce a data frame with a complete tabulation of all the color choices that were picked in our survey.\n\n\nColorsnew <- Colorsnew %>% \n  pivot_longer(everything(),\n  names_to = \"Color\", \n  values_to = \"Tally\") %>%\n  group_by(Color) %>%\n  summarise(Tally = sum(Tally))\n\n\n\n\n\n\n\n“A snapshot of example checkbox survey results separated and tallied together”\n\n\n\n\n\nHave any thoughts or suggestions? Know of a better solution or way to make this process more efficient? Feel free to contact me directly! Respectful discourse towards efficient solutions or new insights is always welcomed!"
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "",
    "text": "Before I write any further, I want to give a special shout out to @ijeamaka_a, @djnavarro, & @jiwanheo who I personally credit for inspiring me within the last month specifically (my only month being an aRtist so far)!\nSo, this post will be a bit different from my others. I’m going to talk about some very basic concepts and perspectives you can think about while starting your own Rtistry journey in ggplot. This includes basics on geoms, aesthetics, layering, etc. But then I’m also going to walk you through two of my Rtistry examples and code to get you started.\nThis article is intended for those who have some experience with ggplot building in R but may not have realized how to transition from making “regular” visuals to Rtistry. This article goes over basic concepts that more seasoned users may already know. If you believe it is too basic, you can jump straight to the code for the finished pieces to see these basics applied together. Seeing how others have constructed things may help you understand how you can apply your programming knowledge to create Rtistry of your own! For those of you that need these basics, I hope this article is helpful and encourages you to start thinking outside of the ggplot grid and start creating your own pieces!\nDon’t want to read and just want the code? Here’s the code for:"
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#introduction",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#introduction",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Introduction",
    "text": "Introduction\nFirst things first: let’s get one thing straight, I am NOT an expert, nor am I a computational scientist or a professional generative artist. While knowledge of some “math things” can be super helpful, you don’t need to know complex computational mathematics to make Rtistry (aRt). Granted, if you enjoy and want to create aRt similar to the works of Thomas Lin Pedersen and Danielle Navarro, then maybe you do! But as a beginner, it’s important to remember that your Rtistry is your work and opportunity to learn, grow as an R programmer, and create pieces you can be proud of. Remember that your aRt is an expression of you and no one else and that there is no “right” way to do Rtistry. One of the things I love most about creating aRt is the fact that I don’t have to get anything “right.” For those of us who use R for work/educational endeavors, we must ensure that our statistics, programming, and literally everything else is correct. Creating Rtistry can provide your mind with not only a creative outlet but a way to still practice programming in R in a low-stress environment where things don’t have to be “right.”\n\nWhy Rtistry?\nSome people may think of the concept of creating aRt and may think it’s a waste of time since no analyses are performed. If you think that way, you have every right to that opinion. What I can share with you, though, is why I personally love creating Rtistry:\n\nGives my brain a rest from “real work”\nAllows me to practice programming skills in a stress-free environment\nGives me satisfaction from flexing my creative skills\nGives me more content for professional portfolios"
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#ethics",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#ethics",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Ethics",
    "text": "Ethics\nWhile using others’ work as inspiration to give yourself ideas is OK, you should always strive to create something of your own merit. Just like in the physical art world, you may also find that some aRtists may not want to share their process or code with you. Those that are new to Rtistry may be shocked by this. Some newbies may even think that creators come off as standoffish or rude if they decline to share their work. This could be because of how supportive the R community usually is and how we are always seemingly ready to share our code.\nOne thing to remember is that a creator could decline to share their code for many reasons. Those decisions are personal and left up to each aRtist on an individual basis. The aRtists that do exceptionally unique work will have usually put a lot of effort and hours into their pieces to a point where it may even feel proprietary. Some aRtists choose to sell their outputs not only because they have made something truly personal to them, but because they’ve also made something unique that probably hasn’t existed before.\nWhen you find an Rtistry piece that appeals to you, you could try to understand what’s happening behind the scenes instead of asking directly for the code. It tends to be a fun challenge to figure out what’s happening yourself. It’s a lot of work, yes, but I am willing to bet that if you use R for professional work, the work and study you do to figure out how to create your pieces will benefit you in the long run.\nWhat you choose to do with the pieces you create is entirely up to you. Some aRtists get their work printed for sale or personal use, some look into selling them as Non-fungible Tokens (NFTs) and some literally save their output and never save their code, completely erasing its footprint in a cathartic click of a button. Whatever you do, always give credit where credit is due; don’t assume that everyone is willing to share their code with you (but if they do, always express gratitude and give credit for the assist); and try to be ethical in everything that you do."
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#geoms",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#geoms",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Geoms",
    "text": "Geoms\nIf you want to start making data aRt in ggplot, having some knowledge on ggplot geoms will prove to be very useful. If you’ve ever participated in a #tidytuesday challenge or have made visuals in ggplot, you’re probably familiar with them. The key to using geoms to create your aRt is to understand what type of data is expected for each geom. Some basic ones to start with are:\n\ngeom_point()\ngeom_line() & geom_path()\ngeom_segment() & geom_curve()\ngeom_polygon()\n\nThere is no limit to which geoms you can use for aRt in ggplot. Your only barrier would be not having your data formatted appropriately for whichever geom you wish to use. For example, if you want to use geom_segment() you’d need to know that this function expects four different data points (x, xend, y, and yend). The geom functions will usually do a good job of throwing an error and letting you know if you’re missing any data points. If you’re interested, you can always learn more about ggplot with the official cheat sheet or book.\n\nGeoms Examples:\nYou can think of basic geoms as instruments (paintbrushes) in your Rtistry toolkit. Here are a few simple examples of how each can be used:\nGeom points can be used for textures, smaller details, or optical illusions like in the image Candy Rope:\n\n\n\nCandy Rope\n\n\n\nGeom line and path can be used to create linear details and abstract aRt :\n\n\n\nLinear Fury\n\n\n\nGeom segment & geom curve can be used for linear/curved details, or larger components:\n\n\n\nMoonscape\n\n\n\nGeom Polygon can be used to make any shape that can exist on ggplot’s coordinate system, Like lots of randomized squares:\n\n\n\nBe Squared"
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#making-and-applying-rtistry-art-data-to-geoms",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#making-and-applying-rtistry-art-data-to-geoms",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Making and Applying (Rtistry) aRt Data to Geoms",
    "text": "Making and Applying (Rtistry) aRt Data to Geoms\nIf the geoms are the instruments in your toolkit, the data is your ink/paint. The data you provide to each geom will produce visuals on ggplot’s coordinate system. Thinking about it broadly, to make one point on a basic ggplot you need at least one x and one y value:\n\nlibrary(tidyverse) #For everything data====\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   1.0.1 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.3.0      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n## Example 1: Data Frame====\nsingle_point <- tibble(x=0, y=0)\n\n## Example 1 ggplot====\nsingle_point %>%\n  ggplot(aes(x = x, y = y))+\n  geom_point()\n\n\n\n\nA Single Point\n\n\n\n\n\nHaving several x and y points is what will get you some shapes, lines, and other things:\n\n## Example 2: Data Frame====\nrandom_lines <- tibble(x = sample(1:500, 50),\n                       y = 1:50)\n\n## Example 2: ggplot====\nrandom_lines %>% \n  ggplot(aes(x=x, y=y, xend = median(x), yend = median(y)))+\n  geom_segment()\n\n\n\n\nRandom Lines\n\n\n\n\n\nThe magic happens when you can start viewing the grid differently. When you begin to understand the relationship the values in a data frame have with the ggplot coordinate system, it becomes easier to realize what data is needed to create your desired output. For example, if you want to create abstract art, you might practice making datasets with randomized values. If you want to make intentional pieces, you might spend time studying the axes of the ggplot and determine what range of values you need to make the figures you want. The process of creating data for Rtistry and applying them to geoms can be as simple or complicated as your creativity allows. Let’s just go over a simple example with a basic square.\nWe know that a square consists of four lines. Each line has a start and endpoint. So, we at least need to make four points on our plot. Since each point consists of two values (x and y), we’ll need eight values in total. Four x points and four y points.\n\n\n## Example 3: Data Frame====\nsquare <- tibble(x = c(0,5,5,0), \n                 y = c(0,0,5,5),\n                 labels = 1:4)\n\n\nBecause we want to make a shape that’s closed (so we could potentially fill it with color), we’d want to use geom_polygon(). You might be thinking “why not use geom_line() instead?” because a square consists of four lines, but let’s look at that. If we make a data set of the points that are supposed to make a square but then pass it through to geom_line() or geom_path() functions, we get these instead:\n\n\n## Example 3: Geom_line() ggplot====\nsquare %>%\n  ggplot(aes(x = x,\n             y = y,\n             label = labels))+\n  geom_line()+\n  geom_label()+\n  coord_equal()\n\n\n\n\n“Square” made with geom_line()\n\n\n\n\n\n## Example 4: Geom_path() ggplot====\nsquare %>%\n  ggplot(aes(x = x,\n             y = y,\n             label = labels))+\n  geom_path()+\n  geom_label()+\n  coord_equal()\n\n\n\n\n“Square” made with geom_path()\n\n\n\n\n\ngeom_line() looks this way because it plots points in order by the x axis. Try to take a moment to compare the data and the result and understand why it came to be.\ngeom_path() looks closer to a square! But we’re missing a side! This is because geom_path() doesn’t create a fully closed shape by default. geom_path() is the same as geom_line() except it plots points in the order in which it appears in the dataset.\nNow let’s use geom_polygon() with the same data set:\n\n\n# Example 5: Geom_polygon ggplot====\nsquare %>%\n  ggplot(aes(x = x,\n             y = y,\n             label = labels))+\n  geom_polygon()+\n  geom_label()+\n  coord_equal()\n\n\n\n\n“Square” made with geom_polygon()\n\n\n\n\n\nThere we go; An actual square. geom_polygon() is similar to how geom_path()’s process of drawing lines works. The only difference is that the start and endpoints are automatically connected and you can add a “fill” aesthetic to your new shape. By default, ggplot does fill the square. You can set fill = NA to make it empty, but remember to set a color argument if you want to see the “border” of the polygon make its shape:\n\n\n# Example 6: Geom_Polygon ggplot with no fill====\nsquare %>%\n  ggplot(aes(x = x,\n             y = y,\n             label = labels))+\n  geom_polygon(fill = NA, color = \"black\")+\n  geom_label()+\n  coord_equal()\n\n\n\n\nSquare - geom_polygon() ggplot with no fill\n\n\n\n\n\nAs a beginner, it’s tempting to try and make every cool idea that comes to you, but it’s a good idea to spend time with these basics first. You can explore creating aRt data sets with formulas you may be interested in or with any of the “non-visual” functions I’ll discuss later in the article. Once you feel comfortable creating aRt datasets and applying them to geoms, you might be able to find cool and unexpected ways to plot things!"
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#aesthetics",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#aesthetics",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Aesthetics",
    "text": "Aesthetics\nThe next important set of basics to learn about is ggplot aesthetics. We’ve already used aesthetics in the previous section’s examples, but let’s think about them a bit more. Each base ggplot and geom function comes with available aesthetics. If you’ve ever made a ggplot visualization and changed any colors, shapes, etc., you’re already familiar with the foundations needed to start customizing the look/style of your work.\nThere’s two ways you can map aesthetics onto your visuals:\n\nThe aes() function in your initial ggplot() function\nDirectly into the geom function you wish to change.\n\nAs you explore what you want to do and how you want to style your data, you may find that you prefer one way over the other. If you get stuck on this part, take some time to think about what you’d like to see on the grid and then think logically about which method would make sense.\nFor example, if you have a variable that contains hex color codes attached to each row in a dataset, it may make sense to set up the aesthetic in the aes() function since it’s already connected to the data. If you have a circumstance where you only want one geom to be one color, or you want to map a different data set of aesthetics to the geom, you can put it into the actual function.\nA word of caution though, if you want to add values from a different dataset, make sure they have the same number of rows as the initial data set that you are piping into the initial ggplot’s function. If they don’t match up and you don’t set the inherit.aes argument to FALSE in the geom, you’ll get an error. There are ways to use different data sets in addition to the initial one used at the beginning of your ggplot call. I’ll touch on this in the layering section of this article. For now, let’s take a quick look into setting aesthetics in our square example:\nLet’s say we want to color our square purple and make colored dots on each point of the square. Because the polygon will only be filled with one color, we can set that in the geom_polygon() call. If we add a color variable to our data set with individual hex color codes attached to each observation, we can place it in the aes() function and get our desired output with the following code:\n\n\n# Example 7: Geom_Polygon ggplot with colored points====\n\n## Example 7: Data Frame====\nsquare_color_points <- tibble(x = c(0,5,5,0),\n                 y = c(0,0,5,5),\n                 colors = c(\"#c0d943\",\"#027337\",\"#d12017\",\"#000a96\"))\n                 \n## Example 7: Geom_Polygon ggplot with colored points====\nsquare_color_points %>%\n  ggplot(aes(x = x,\n             y = y))+\n  geom_polygon(fill = \"#4b1980\")+\n  geom_point(color = square_color_points$colors)+\n  coord_equal()\n\n\n\n\nSquare- geom_polygon() with colored points\n\n\n\n\n\nWe can also change multiple aesthetics at once. We can change the size of our points too:\n\n\n# Example 8: Geom_Polygon ggplot with colored points and Random Sizes====\nsquare_color_points %>%\n  ggplot(aes(x = x,\n             y = y))+\n  geom_polygon(fill = \"#4b1980\")+\n  geom_point(color = square_color_points$colors,\n             size = sample(1:20, nrow(square_color_points)))+\n  coord_equal()\n\n\n\n\nSquare - geom_polygon() with randomly sized points\n\n\n\n\n\nAs you get more comfortable with aesthetics, you will find that there may be different ways to explore these settings programmatically using various ideas or functions. One of the greatest things about Rtistry is realizing that you may already have the programming knowledge necessary to use these basic components to create amazing things.\nAt the end of the day, if you find yourself stuck on how to do something, it may be helpful to search google, the RStudio Community forums, or even Stack Overflow for the answers to programming questions that may help you piece together your Rtistry. In my experience, I’ve always had the best results when I take a moment to stop and think about what I am asking R to do programmatically. R doesn’t know that I want to randomize colors for an image, but it will know if I want it to pick random hex color values from a vector by using the sample() function. I can then apply that result to an aesthetic option and get the desired output.\nIf I were googling for the answer to this example, I wouldn’t type “how to randomize colors of geom_point()” but instead I’d type “how to randomly pick values from a vector in R programming.” It may take a little bit to understand this thought process, but you are programming after all, so try to approach any troubleshooting as if you were coding for “actual work”."
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#using-non-visual-functions-for-your-visuals",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#using-non-visual-functions-for-your-visuals",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Using “non-visual” Functions For Your Visuals",
    "text": "Using “non-visual” Functions For Your Visuals\nThe first time I realized that you could pass additional functions into aesthetic options - my mind was blown! I did not realize that anything other than a string of a hex color could go into a color aesthetic in ggplot. My mind was so stuck on how I learned to code for ggplot. It was also bound by the fact that I never needed to think about setting any options outside of regular color “strings” for my everyday work. Take the random data below. We could change the colors of these random points by applying some random colors…\n\n\n## Example 9: Data Frame====\nrandom_points <- tibble(x = 1:100,\n                        y = sample(1:100))\n\n## Example 9: ggplot of randomized points====\nrandom_points %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point()\n\n\n\n\nGgplot of randomized points\n\n\n\n\n\n\n## Example 10: ggplot of randomized points with randomized colors====\nrandom_points %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point(color = sample(RColorBrewer::brewer.pal(5,\"PRGn\"), nrow(random_points), replace = TRUE))\n\n\n\n\nGgplot of randomized points with randomized colors\n\n\n\n\n\n…Or by placing a logical function in the color aesthetic for geom_point(). For example, we can tell R that we only want to apply randomized colors to any data points that have a y value that’s less than 50, otherwise observations with a y value that’s greater than or equal to 50 will just be colored black with an ifelse() function:\n\n\n# Example 11: ggplot of randomized points with randomized colors and logic applied====\nrandom_points %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point(color = ifelse(random_points$y < 50,\n                            sample(RColorBrewer::brewer.pal(5,\"PRGn\"), nrow(random_points), replace = TRUE),\n                            \"black\"))\n\n\n\n\nGgplot of randomized points with randomized colors and logic applied\n\n\n\n\n\nSince there is no limit to the different types of functions a user can make, the possibilities of different functions that can be added to aesthetics seems endless. A few functions that might help you not only in aesthetics, but working with your data prepping are listed below to get you started:\n\n\nsample() – Randomly take a sample from elements of an object\nrep() – Replicate (repeat) elements in a vector or list\nseq() – Generates numeric sequences\nrunif() – Returns a random composition of numbers with a uniform distribution\ndnorm() & rnorm() – Creates a normal density distribution and random normal distribution of numbers\nTrigonometric Functions (cospi(),cos(),sin(), etc.) – Results in the computation of various trigonometric functions.\nCrossing(), expand(), nesting() – Tidyr functions that expand data frames to include possible combinations of values.\nControl-flow constructs (while, repeat,if, for, etc.) – Can be used to create iterations, patterns, and functions of your own.\nApply family functions (lapply, sapply, etc.) – Can be used to apply functions on different objects like matrices, vectors, or lists.\n\n\nRemember that this is not an exhaustive list, but just a taste of some commonly used functions to hopefully get the gears turning about different ways you can manipulate your ggplot aesthetics and actual Rtistry data."
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#layering",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#layering",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Layering",
    "text": "Layering\nThe last basic concept in this post is ggplot layering. Adding layers with the layer() function or even annotations with the annotate() function is very useful as it allows the user to add different layers of data. You can also achieve this by adding geom functions as you would normally, but if you ever want to add geoms that pull from different data sources this way, you need to set the inherit.aes argument to FALSE, as this will then allow the ggplot to build without error.\nThese multiple ways of layering allows for endless possibilities as piecing together an image this way provides a bit more flexibility. Take the two datasets below. One is intended to just create some lines with geom_segment(), and the other is intended to just create circles in between each line with geom_point(). Because we want 4 lines and 3 circles, the amount of data needed to map these things don’t match up. We can tell this on a quick glance by seeing that the number of rows in each data set don’t match up:\n\n\n## Example 12: Lines_data Data Frame====\nLines_data <-  tibble(x = rep(1, 4),\n                      xend = rep(5, 4),\n                      y = seq(0,6, by = 2),\n                      yend = y )\n\n## Example 12: Circles_data Data Frame====\nCircles_data <- tibble(x = 3,\n                       y = unique(abs(seq(0,6, by = 2) - 1)),\n                       size = 1:3)\n\n##Logic Check: Do these dataframes have the same number of row?##\nnrow(Lines_data) == nrow(Circles_data)\n\n[1] FALSE\n\n\n\nIf we try to add these together by just using a geom function ,geom_point(), and supplying the Circles_data directly into geom_point() without typing inherit.aes = FALSE, the console will throw an error:\n\n\n## Example 12: Lines_data and Circles_data error====\nLines_data %>%\n  ggplot(aes(x=x, y=y, xend = xend, yend = yend))+\n  geom_segment(size = 15)+\n  geom_point(data = Circles_data, aes(x=x, y=y, size = size)) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\ni Error occurred in the 2nd layer.\nCaused by error in `FUN()`:\n! object 'xend' not found\n\n\n\nThis error tells you that object xend is not found. This is because we are attempting to override the data frame that is attached to the entire ggplot (Lines_data) with Circles_data now and xend definitely does not exist in Circles_data and therefore “can’t be found.” To fix this specific example and get the desired results, simply add inherit.aes = FALSE into the geom_point() function. If desired, you also could just move the aesthetics for Lines_data into the geom_segment() function but as you will find, there are multiple ways to get to the same result. For this example though, we’ll just add the inherit.aes argument to geom_point().\n\n\n## Example 12: Adding Lines_data and Circles_data together with inherit.aes in geom====\n\nLines_data %>%\n  ggplot(aes(x=x, y=y, xend = xend, yend = yend))+\n  geom_segment(size = 15)+\n  geom_point(data = Circles_data, aes(x=x, y=y, size = size), inherit.aes = FALSE)\n\n\n\n\nggplot of Lines_data and Circles_data layered together with the addition of geom_point()\n\n\n\n\n\nWe can also use the layer() function or the annotate() function to achieve similar results. Both methods are used in the example below:\n\n\n# Example 13: Adding Lines_data and Circles_data together with layer function====\nLines_data %>%\n  ggplot(aes(x=x, y=y, xend = xend, yend = yend))+\n  geom_segment(size = 15)+\n  layer(geom = \"point\",\n        data = Circles_data,\n        stat = \"identity\",\n        position = \"identity\",\n        mapping = aes(x=x, y=y, size = size),\n        inherit.aes = FALSE)\n\n\n\n\nggplot of Lines_data and Circles_data layered together with the layer() function\n\n\n\n\n\n\n# Example 14: Adding Lines_data and Circles_data together with annotate function====\nLines_data %>%\n  ggplot(aes(x=x, y=y, xend = xend, yend = yend))+\n  geom_segment(size = 15)+\n  annotate(geom = \"point\",\n           x= Circles_data$x, \n           y= Circles_data$y, \n           size = Circles_data$size)\n\n\n\n\nggplot of Lines_data and Circles_data layered together with the annotate() function\n\n\n\n\n\nBoth functions produce similar images. The layer() and annotate() functions have their own pros and cons based on what you’d like to do. As shown in the code snippets, each method of layering in ggplot is similar but has its own differences that may be favored in certain situations. I’d encourage you to read and explore these methods of layering a bit more, and to see what works best to execute your ideas."
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#planet-quad-art-example",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#planet-quad-art-example",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Planet Quad – aRt Example",
    "text": "Planet Quad – aRt Example\n \n\n\n\nPlanet Quad\n\n\n\n\nNow that we’ve gone over some technical basics, let’s look at two Rtistry examples I’ve created. The first is Planet Quad. The creation of Planet Quad demonstrates how we can utilize other packages like patchwork and cowplot to create our own Rtisrty. Let’s break this down step-by-step.\nEach layer in ggplot is built on top of each other in the order in which it appears in the code. For this image, that means we want the space/stars background to be behind our planets- so we can start with that first.\nWe want four different plots with a similar design in different colors, so we can start by creating a named vector of the background colors we want to use. I always use hex colors, but you can use whatever format you’d like. I’ll name mine Space_colors:\n\n\n# Library Load-In====\nlibrary(tidyverse) # For everything data\nlibrary(cowplot) # For placing plots on top of each other easily with ggdraw and draw_image\nlibrary(patchwork) # For packaging up the plots in the final image\n\nWarning: package 'patchwork' was built under R version 4.1.3\n\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:cowplot':\n\n    align_plots\n\n# Star Backgrounds====\n\n## Making four different stars \"backgrounds\" with four different colors====\nSpace_colors <- c(\"Topleft\"=\"#000000\",\n                  \"Topright\"=\"#130321\",\n                  \"Bottomleft\"=\"#330500\",\n                  \"Bottomright\"=\"#8a4500\")\n\n\nNext, we want to create the data for the stars in our backgrounds. Because I want the image to look pretty filled in, I’ll apply the crossing() function to the data in this tibble that I’ll name stars:\n\n\n# Making a data set that will be used to create all star backgrounds====\nstars <- tibble(crossing(x = seq(1,2000,100),\n                         y = seq(1,2000,100)))\n\n\nNext, we want to make a list because we’ll run iterations of our stars dataset through a ggplot call that will also iterate through our vector Space_colors. This will result in a list of four similar plots that all have a different color scheme. We’ll name this list All_stars and name the plots inside of the list to make it easier to track each plot as our session continues:\n\n\n## Making the list to hold the \"space\" plots====\nAll_stars <- list()\n\n## Iterating through the \"space_colors\" to make four different plots====\nfor(i in seq_along(Space_colors)){\n  \n  All_stars[[i]] <- stars %>%\n    ggplot(aes(x = x, y = y))+\n    geom_jitter(size = sample(c(.02,.04,.06,.8),nrow(stars), replace = TRUE), color = \"white\")+\n    theme_void()+\n    theme(plot.background = element_rect(fill = Space_colors[i], color = \"#ffffff\", size = 6)) \n}\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\ni Please use the `linewidth` argument instead.\n\n## Naming the plots just to help keep track of what's going where====\nnames(All_stars) <- names(Space_colors)\n\n\nNow we can check and look at one of our plots by typing All_stars$Topleft in the console to verify that it looks like what we were expecting:\n\nAll_stars$Topleft\n\n\n\n\nThe “Topleft” image in the All_stars list. A ggplot of the stars data\n\n\n\n\n\nNow, we can work on our planets. The planets are actually just a geom_density2d_filled() plot that will go through four iterations of the planet data set with its coordinate system set to [coord_polar()](https://ggplot2.tidyverse.org/reference/coord_polar.html. Just like we did for the stars data, we also want four very similar plots with different colors. Again, we can make a similar function to create a list of these plots. We just need to generate four different iterations of the data set and pick some colors for the planet surfaces. Let’s start with picking color palettes for planets and different colors for the border lines of each planet. We can place each palette into a list called Planet_colors and each border color into a vector called Planet_borders:\n\n\n# Planet Creations====\n## Setting the color palettes for each planet quad====\nPlanet_colors <- list(\"Topleft\" = c(\"#194157\",\"#008dd7\",\"#085b88\",\"#26925e\",\"#095c88\"),\n                      \"Topright\" = c(\"#480463\",\"#392242\",\"#1e0329\",\"#bf9232\",\"#120b17\"),\n                      \"Bottomleft\" = c(\"#47322d\",\"#6b1c09\",\"#a30000\",\"#6b1d09\",\"#851205\"),\n                      \"Bottomright\" = c(\"#c7a602\",\"#998523\",\"#ba690d\",\"#755b3d\",\"#dbab39\"))\n\n## Setting the colors of each planet's borders====\nPlanet_borders <- c(\"Topright\" = \"#333333\",\n                    \"Topright\" = \"#120b17\",\n                    \"Bottomleft\" = \"#260f09\",\n                    \"Bottomright\" = \"#5c523a\")\n\nWe’ll create some randomly sampled data to draw out our planets to encourage some weirdness in its final presentation. We’ll name the dataset planet:\n\n\n# Making a dataset that will be used to create all the planets====\nplanet <- tibble(crossing(x = sample(1:1000,100, replace = TRUE),\n               y = sample(1:2000, 100, replace = TRUE)))\n\n\nNow we’ll create our list, named All_planets, that will be filled with our four different planets after it runs through our for-loop:\n\n\n## Making the list to hold the \"planet\" plots====\nAll_planets <- list()\n\n## Iterating through the \"planet_colors\" to make four different planet plots====\nfor(i in seq_along(Planet_colors)){\n  \n  All_planets[[i]] <- planet %>%\n    ggplot(aes(x = x, y = y))+\n    scale_fill_manual(values = sample(Planet_colors[[i]],100, replace = TRUE))+ #100 is just a arbitrary \"safe\" number I picked. geom_density does background calcs to create levels that varies based on data.\n    geom_density2d_filled(color = Planet_borders[i], size = 2)+\n    coord_polar(clip = \"on\")+\n    theme_void()+\n    theme(legend.position = \"none\")\n}\n\n## Naming the plots just to help keep track of what's going where====\nnames(All_planets) <- names(Planet_colors)\n\n\nAll these plots are now in our list All_planets. We can peek at one by typing All_planets$Bottomright into the console. Because a seed hasn’t been set anywhere, yours may look different than mine and that’s all right!\n\nAll_planets$Bottomright\n\n\n\n\nThe “Bottomright” image in the All_stars list. A ggplot of the stars data\n\n\n\n\n\nNow we can save all these planet plots as images out into the directory so we can load them right back in as png files. Please note that if you are following along, you may have to alter the directory to avoid errors as this directory is located within my Data_aRt GitHub repo in my Planet Quad project file.\n\n\n# Saving Planets into the directory====\nfor(i in seq_along(All_planets)){\n  \nggsave(paste0(\"planets/\",\n              names(All_planets)[i],\n              \"_planet.png\"),\n       All_planets[[i]],\n       bg = \"transparent\", \n       device = \"png\")\n}\n\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\n\n\n  \n\n\n\nA snapshot of the processed PNG files in my personal working directory locally.\n\n\n\n\nNext, we need to bring the processed png files back into the environment. We can do this with the png package. Because I’m only using this once, I chose to call in the readPNG() function directly from the package instead of loading the package itself. We’ll store the images in a list called Planet_PNGs.\n\n\n# Loading Planets back into the environment as PNGs====\nPlanet_PNGs <- list()\n\nfor(i in seq_along(All_planets)){\n  \nPlanet_PNGs[[i]] <- png::readPNG(paste0(\"planets/\",\n                                        names(All_planets)[i],\n                                        \"_planet.png\"))\n}\n\n## Setting names to keep track of the planets====\nnames(Planet_PNGs) <- names(All_planets)\n\n\nNow that we have png files, we can place these “on top” of our star backgrounds we made earlier using the ggdraw() and draw_image() functions from the cowplot package with for-loops. This time, we’ll store them in a list called Combined_plots:\n\n\n# Combining both the stars and planets to create four plots in total====\nCombined_plots <- list()\n\nfor(i in seq_along(All_planets)){\n  \n  Combined_plots[[i]] <- ggdraw(All_stars[[i]]) +\n                                  draw_image(Planet_PNGs[[i]])\n}\n\n## Setting names to keep track of the plots===\nnames(Combined_plots) <- names(Planet_PNGs)\n\n\nJust like before, we can take a peek at any of the plots in the Combined_plots list to make sure it looks as expected. I’ll use Combined_plots$Bottomleft in the console to see:\n\nCombined_plots$Bottomleft\n\n\n\n\nThe stars and planet plots combined\n\n\n\n\n\nLooking good! Almost done! Now we can pluck() each of our plots out and save them to individual objects! Because there’s only four of them, I’ll just copy and paste that code because this is Rtistry and it’s totally OK to be lazy. It’s your work. It’s your aRt . Do what you want!\n\n\n## Plucking out all the individual plots===\nTopleft <- Combined_plots %>% pluck(\"Topleft\")\nTopright <- Combined_plots %>% pluck(\"Topright\")\nBottomright <- Combined_plots %>% pluck(\"Bottomright\")\nBottomleft <- Combined_plots %>% pluck(\"Bottomleft\")\n\n\nFinally, we use pacthwork’s layout syntax to combine these plots into one image:\n\n\n# Final output construction with patchwork functions====\nPlanet_Quad <- (Topleft + Topright) / (Bottomleft + Bottomright)\n\n# View the piece#\nPlanet_Quad\n\n\n\n\nPlanet Quad - Finished\n\n\n\n\n\nAnd voila! We have our finished piece! I’ve noticed that sometimes patchwork may render in some borders around these images if ggsave() is used. In these cases, I just export using the Export button in the RStudio IDE."
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#rainbow-rose-art-example",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#rainbow-rose-art-example",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Rainbow Rose – aRt Example",
    "text": "Rainbow Rose – aRt Example\n \n\n\n\nRainbow Rose\n\n\n\n\nFinally, we’re at our last example! Rainbow Rose is an Rtistry piece that showcases one way trigonometric functions can be used to create cool results. This Rtistry piece was created from a beautiful accident when I mistyped some code while attempting to draw a circle from scratch in ggplot (yes, the geom_circle() function from ggforce exists, but I was trying to improve my manual ggplot skills 🤷🏾️).\n\nTo start out, we need to set up some specifications that will alter how the data is plotted onto ggplot’s Cartesian coordinate system. This includes calculating the “theta”(angles) of our intended circle, the number of divisions/data points we want in the dataset “n”, and the radial setting we wish to have for the visual. Because we want to give an effect that starts from the center of the image, we set the radial setting, “r” to a vector that spans from 1 to our “n” value.\n\n\n# Library Load-In====\nlibrary(tidyverse) # For everything data\n\n# Setting parameters to prep ggplot to plot data in a \"circular\" fashion on the Cartesian coordinate system====\n\n## Angle \"slices\"/ Sine/Cosine Frequency====\ntheta <- seq(0, 40*pi, length = 100) \n\n## Number of divisions/rows in the data wanted====\nn <- 500\n\n## \"Radial\" setting of the \"circle\" to create \"n\" different marks====\nr = 1:n\n\n\nNext, I’ll use one of my favorite color palettes that I found for one of my #TidyTuesday submissions. I’ll store them in a vector simply called “colors”:\n\n\n## Setting up the custom color palette====\ncolors <- c(\"#af3918\", \"#a21152\", \"#822b75\",\"#612884\",\"#154baf\",\n            \"#0b82b9\", \"#277e9d\",\"#488e35\",\"#e3a934\",\"#b2336a\")\n\n\nNow I’ll create a simple data frame. I’ll create my x and y variables using the cos() and sin() functions respectively:\n\n\n# Placing everything into a dataset====\ndata <- tibble(x = cos(theta)*r,\n               y = sin(theta)*r)\n\n\nFinally, we’ll put everything together. Using geom_path() on this dataset and passing our colors vector into it creates our Rainbow Rose!\n\n\n# Pulling it all together====\nRainbow_Rose <- data %>%\n  ggplot(aes(x = x, y = y, color = color))+\n  geom_path(color = rep(colors, each = n/10), size = 1)+\n  theme_void()+\n  theme(legend.position = \"none\",\n        panel.background = element_rect(fill = \"black\"))\n\n# View it #\nRainbow_Rose\n\n\n\n\nRainbow Rose - Finished\n\n\n\n\n\nMesmerizing! Play around and investigate the options used to try and understand how/why ggplot generated the data this way.\nFor example, if we change geom_path() to geom_line() we get this:\n\n\n\n\n\n\nRainbow Rose with geom_line() instead of geom_path()\n\n\n\n\n\nStill pretty, but a completely different piece! Earlier in the article I mentioned the difference between geom_path() and geom_line() . Given their differences, does it make sense why this version looks this way? These are just some of the questions you can think about and ask yourself while you’re on your Rtistry journey!"
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#theres-a-package-for-that",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#theres-a-package-for-that",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "There’s a Package for That",
    "text": "There’s a Package for That\nIf you read all of this and still feel intimidated, that’s OK. You can still try to get into Rtistry with some of the following packages: \n\naRtsy developed by Koen Derks “Aims to make generative aRt accessible to the general public in a straightforward and standardized manner”\nAmbient developed by Thomas Lin Pedersen and Jordan Peck(FastNoise) “…is a an R package that provides access to the FastNoise C++ library for generating noise.”\nGgvoroni developed by Robert Garrett and Thomas Fisher “With ggvoronoi we can easily draw Voronoi diagram heatmaps, with help from packages deldir and ggplot2.”\ncontouR developed by Ijeamaka Anyene “contouR is a package that is a wraparound for ggplot2::geom_contour() to use for generative aRt .”\nJasmines developed by Danielle Navarro “The jasmines package is what Danielle uses to make a lot of her generative artwork with R. Internally it relies heavily on the ambient package, and you may need to install the developer version of ambient to make it work.”"
  },
  {
    "objectID": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#final-remarks",
    "href": "post/thinking-outside-the-grid/thinking-outside-the-grid.html#final-remarks",
    "title": "Thinking Outside The Grid - A “bare bones” intro to Rtistry concepts in R using ggplot.",
    "section": "Final Remarks",
    "text": "Final Remarks\nThis concludes The Tidy Trekker Intro to Data Art post! I really hope that those of you who are just trying to start out in Rtistry found this to be useful. While the basics content truly is basic, it really is a good chunk of the foundation needed to eventually progress into more complex generative aRt . Sometimes it can be difficult to disseminate the how-tos or guides for a creative process. As you saw with the Rainbow Rose piece, sometimes aRt can be created by accident. Those accidents can only be unique to you. The best way to improve your Rtistry skills is to just jump in and play. Use the time to zone out, relax, and engage with data in a way you may not have before.\nHave you started your Rtistry journey? Do you think some crucial basics were left out of this post? Feel free to contact me directly! Respectful discourse is always welcome!"
  },
  {
    "objectID": "post/where-have-i-been/where-have-i-been.html",
    "href": "post/where-have-i-been/where-have-i-been.html",
    "title": "Where Have I Been?",
    "section": "",
    "text": "So, it has been a good five months since I last made a post on The Tidy Trekker. I can assure you that I am alive and kicking! Though I know I do not have any obligations or posting schedules, I would prefer not to go that long without posting anything at all. I know it’ll happen sometimes, but I love adding content here and interacting with others about it over on Twitter (you can follow me there if you haven’t already, it’s a good time! Lots of data, lots of memes, and lots of R stuff).\nI just wanted to make a more low-key, short, and personal post to take a moment, stop, and say hello to any readers out there. How are you guys? How is the pandemic treating you on your end of the universe? Well, for me, it’s been an interesting challenge, and by “interesting” I mean kind of sucky.\n\n\n\n\n\n\nLife sucks sometimes. It’s OK to say it.\n\n\n\n\n\nI had a lot of challenges, setbacks, and downright painful things thrown at me this year. These things threw me off track for a bit. While I’m not ready to talk about everything, I can touch on a few things. My last Tidy Trekker post was in April. Around this time, I was finishing up my TarotreadR app for the Shiny contest this year. I actually just wrote up a post about that experience as it was the first Shiny app I ever made. You can read more about that here if you’d like.\nDuring all of this, I was also hospitalized for a bit but then was able to go to Rochester (NY) for my birthday when I got out. When I got back, my efforts moved towards wrapping up some projects at work as I was preparing to leave the country in August for my wedding anniversary. While I was doing that, I finally landed a second job as a contracted Data Analyst (finally, some good stuff)!\nI eventually got through work, and a whole bunch of other responsibilities, to finally receive my reward: an anniversary trip to Mexico. When I stepped foot onto the resort, I nearly wept. I was so grateful and privileged to be able to just escape everything for a bit given the year I’d been having.\n\n\n\n\n\n\n\n\nOne of the gifts my husband gave me in Mexico.\n\n\n\n\n\nBut now vacation is over and I’m back in reality. Having the time away was really important for me to think about the next steps I’d like to consider for my career. Currently, I’m still in love with my position and have been learning so much and am so grateful for it. Moving forward, I’d like to focus more on a few things:\n\n\nStrengthening my knowledge of statistical methodology\nWorking on improving the efficiency of my programming in general\nGetting familiar with Python and SQL\nTrying to complete a tiny ML project before the year is up\n\n\nI plan to use The Tidy Trekker to document these endeavors, but given that I am starting a contracted position, I will have to play everything by ear of course.\nHow has your 2021 been so far? How’s life? Got any updates you want to share? Feel free to contact me directly! Respectful discourse is always welcomed!"
  },
  {
    "objectID": "rtistry.html",
    "href": "rtistry.html",
    "title": "Rtistry Gallery",
    "section": "",
    "text": "“Rtistry” is a term that is used to refer to generative art that’s created by coding in the R programming language. Here you’ll find various Rtistry pieces I’ve created as I continue to explore and learn how to create aRt. I have publicly shared some code for certain pieces in my Data Art repository on GitHub. If you’re a beginner interested in getting started in Rtistry, you may find my Intro to Data Art blog post helpful.\n\n\n\n\n\n\n‘Be Squared’\n\n\n\n,\n\n\n\n‘Blue Waves’\n\n\n\n,\n\n\n\n‘Buildings’\n\n\n\n,\n\n\n\n‘Candy Ball’\n\n\n\n,\n\n\n\n‘Candy Rope’\n\n\n\n,\n\n\n\n‘City Limits’\n\n\n\n,\n\n\n\n‘crossover’\n\n\n\n,\n\n\n\n‘Egg’\n\n\n\n,\n\n\n\n‘Fall Flower’\n\n\n\n,\n\n\n\n‘Glass Ball’\n\n\n\n,\n\n\n\n‘Glass Tornado’\n\n\n\n,\n\n\n\n‘Groovy’\n\n\n\n,\n\n\n\n‘Manic Panic’\n\n\n\n,\n\n\n\n‘Moonscape’\n\n\n\n,\n\n\n\n‘Mountains’\n\n\n\n,\n\n\n\n‘Planet Quad’\n\n\n\n,\n\n\n\n‘Polar Wave’\n\n\n\n,\n\n\n\n‘Polycyst’\n\n\n\n,\n\n\n\n‘Rainbow Rose’\n\n\n\n,\n\n\n\n‘Red Swirl’\n\n\n\n,\n\n\n\n‘Snazzy Triangles’\n\n\n\n,\n\n\n\n‘String Balls’\n\n\n\n,\n\n\n\n‘Swirl’\n\n\n\n,\n\n\n\n‘Trees’\n\n\n\n,\n\n\n\n‘Waterstones’\n\n\n\n,\n\n\n\n‘Wave Patch’"
  },
  {
    "objectID": "talks/collages_and_patterns/collages_and_patterns.html#description",
    "href": "talks/collages_and_patterns/collages_and_patterns.html#description",
    "title": "Collages and Patterns: Making Art in R",
    "section": "Description",
    "text": "Description\n\nThis talk will briefly introduce the concept of creating generative art in R (Rtistry) but then focus on a concept that seems to allude most: how to create the data required for generative art. While knowledge of generative art is not required to attend, this talk does assume that attendees are comfortably familiar with the tidyverse, especially ggplot2. Attendees will learn some basic concepts involved in creating collages and patterns in ggplot2 by focusing on the logic required to generate randomized and non-randomized data sets for their rtistry! All scripts, slides, and talk materials will be available for use and review at the start of the talk."
  },
  {
    "objectID": "talks/collages_and_patterns/collages_and_patterns.html#slides",
    "href": "talks/collages_and_patterns/collages_and_patterns.html#slides",
    "title": "Collages and Patterns: Making Art in R",
    "section": "Slides",
    "text": "Slides\nThe slides for the presentation is available on GitHub here."
  },
  {
    "objectID": "talks/collages_and_patterns/collages_and_patterns.html#recorded-presentation",
    "href": "talks/collages_and_patterns/collages_and_patterns.html#recorded-presentation",
    "title": "Collages and Patterns: Making Art in R",
    "section": "Recorded Presentation",
    "text": "Recorded Presentation\nThis presentation was recorded and is now accessible on YouTube here"
  },
  {
    "objectID": "talks/collages_and_patterns/collages_and_patterns.html#talk-materials-and-github-repository",
    "href": "talks/collages_and_patterns/collages_and_patterns.html#talk-materials-and-github-repository",
    "title": "Collages and Patterns: Making Art in R",
    "section": "Talk Materials and GitHub Repository",
    "text": "Talk Materials and GitHub Repository\nThis talk has a supplemental GitHub repository that can be accessed here"
  },
  {
    "objectID": "talks/exploring_validation/exploring_validation.html#description",
    "href": "talks/exploring_validation/exploring_validation.html#description",
    "title": "Exploring Data Validation With The pointblank Package",
    "section": "Description",
    "text": "Description\n\nThis talk will provide information on what data validation is, why it’s important, and how R users can begin using the pointblank package to validate their own data in R. Participants can read more about the pointblank package here"
  },
  {
    "objectID": "talks/exploring_validation/exploring_validation.html#slides",
    "href": "talks/exploring_validation/exploring_validation.html#slides",
    "title": "Exploring Data Validation With The pointblank Package",
    "section": "Slides",
    "text": "Slides\nThe slides for the presentation is available on GitHub here."
  },
  {
    "objectID": "talks/exploring_validation/exploring_validation.html#recorded-presentation",
    "href": "talks/exploring_validation/exploring_validation.html#recorded-presentation",
    "title": "Exploring Data Validation With The pointblank Package",
    "section": "Recorded Presentation",
    "text": "Recorded Presentation\nThis presentation was recorded and is now accessible on YouTube here"
  },
  {
    "objectID": "talks/exploring_validation/exploring_validation.html#talk-materials-and-github-repository",
    "href": "talks/exploring_validation/exploring_validation.html#talk-materials-and-github-repository",
    "title": "Exploring Data Validation With The pointblank Package",
    "section": "Talk Materials and GitHub Repository",
    "text": "Talk Materials and GitHub Repository\nThis talk has a supplemental GitHub repository that can be accessed here"
  },
  {
    "objectID": "talks/functions_in_rtistry/functions_for_rtistry.html#description",
    "href": "talks/functions_in_rtistry/functions_for_rtistry.html#description",
    "title": "Making Functions For Rtistry",
    "section": "Description",
    "text": "Description\n\nWhile you don’t have to be an R expert to attend, some of the material expects that the viewer has a basic understanding of using the ggplot2 package and functional programming basics in R. This talk will go over the benefits of using functions from Base R and the Purrr package, different types of functions that can be created for generative art, and more. The workshop will end with a live coding rtistry session that viewers can follow along with. The code created in the live coding session will be available shortly after the session on GitHub."
  },
  {
    "objectID": "talks/functions_in_rtistry/functions_for_rtistry.html#slides",
    "href": "talks/functions_in_rtistry/functions_for_rtistry.html#slides",
    "title": "Making Functions For Rtistry",
    "section": "Slides",
    "text": "Slides\nThe slides for the presentation is available on GitHub here."
  },
  {
    "objectID": "talks/functions_in_rtistry/functions_for_rtistry.html#recorded-presentation",
    "href": "talks/functions_in_rtistry/functions_for_rtistry.html#recorded-presentation",
    "title": "Making Functions For Rtistry",
    "section": "Recorded Presentation",
    "text": "Recorded Presentation\nThis presentation was recorded but the link has not been provided by R-Ladies Johannesburg yet."
  },
  {
    "objectID": "talks/functions_in_rtistry/functions_for_rtistry.html#talk-materials-and-github-repository",
    "href": "talks/functions_in_rtistry/functions_for_rtistry.html#talk-materials-and-github-repository",
    "title": "Making Functions For Rtistry",
    "section": "Talk Materials and GitHub Repository",
    "text": "Talk Materials and GitHub Repository\nThis talk has a supplemental GitHub repository that can be accessed here"
  },
  {
    "objectID": "talks/intro_to_rtistry/intro_to_rtistry.html#description",
    "href": "talks/intro_to_rtistry/intro_to_rtistry.html#description",
    "title": "An Introduction to Rtistry Using ggplot2 in R",
    "section": "Description",
    "text": "Description\n\nIn this introductory presentation, we will briefly go over the basics of creating generative art in R (Rtistry) using the ggplot2 package. This presentation will go over the common geoms (geometries), aesthetics, layering, and functions that can be used for Rtistry creation. The presentation will conclude with a live code review session of one Rtistry pieces. Full annotated code will be shared with participants for further practice and use."
  },
  {
    "objectID": "talks/intro_to_rtistry/intro_to_rtistry.html#slides",
    "href": "talks/intro_to_rtistry/intro_to_rtistry.html#slides",
    "title": "An Introduction to Rtistry Using ggplot2 in R",
    "section": "Slides",
    "text": "Slides\nThe slides for the presentation is available in a PDF format here."
  },
  {
    "objectID": "talks/intro_to_rtistry/intro_to_rtistry.html#recorded-presentation",
    "href": "talks/intro_to_rtistry/intro_to_rtistry.html#recorded-presentation",
    "title": "An Introduction to Rtistry Using ggplot2 in R",
    "section": "Recorded Presentation",
    "text": "Recorded Presentation\nThe recorded presentation for this talk is available on YouTube here."
  },
  {
    "objectID": "talks/intro_to_rtistry/intro_to_rtistry.html#github-repository-with-example-code",
    "href": "talks/intro_to_rtistry/intro_to_rtistry.html#github-repository-with-example-code",
    "title": "An Introduction to Rtistry Using ggplot2 in R",
    "section": "GitHub Repository with Example Code",
    "text": "GitHub Repository with Example Code\nThe example R script for the live-coded piece for this talk is available on GitHub here."
  },
  {
    "objectID": "talks/pipelines_in_r/pipeline_in_r.html#description",
    "href": "talks/pipelines_in_r/pipeline_in_r.html#description",
    "title": "Making Data Pipelines in R: A Story from a Self-Taught Perspective",
    "section": "Description",
    "text": "Description\n\nR’s expansive capabilities can leave some feeling overwhelmed when tasked with larger projects like data pipelines. This talk invites the participant to hear the perspective of a self-taught R user who used curiosity and patience to create a functional data pipeline in R for a local health department. Specifically, this talk will touch on the following concepts:\n\n\nSurveying Data Landscapes\nFile Structures\nSaving Yourself with Data Validation\nModularizing Code and Connecting R Scripts\nThinking about Pipeline Sustainability\nRemaining Calm in Unfamiliar R Territories”"
  },
  {
    "objectID": "talks/pipelines_in_r/pipeline_in_r.html#slides",
    "href": "talks/pipelines_in_r/pipeline_in_r.html#slides",
    "title": "Making Data Pipelines in R: A Story from a Self-Taught Perspective",
    "section": "Slides",
    "text": "Slides\nThe slides for the presentation is available on GitHub here."
  },
  {
    "objectID": "talks/pipelines_in_r/pipeline_in_r.html#recorded-presentation",
    "href": "talks/pipelines_in_r/pipeline_in_r.html#recorded-presentation",
    "title": "Making Data Pipelines in R: A Story from a Self-Taught Perspective",
    "section": "Recorded Presentation",
    "text": "Recorded Presentation\nThis presentation was recorded and is now accessible on the RStudio::conf(2022) website here"
  },
  {
    "objectID": "talks/pipelines_in_r/pipeline_in_r.html#talk-materials-and-github-repository",
    "href": "talks/pipelines_in_r/pipeline_in_r.html#talk-materials-and-github-repository",
    "title": "Making Data Pipelines in R: A Story from a Self-Taught Perspective",
    "section": "Talk Materials and GitHub Repository",
    "text": "Talk Materials and GitHub Repository\nThis talk has a supplemental GitHub repository that can be accessed here"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "The Tidy Trekker",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nExploring Data Validation With The pointblank Package\n\n\n\n\n\nData validation is a very important component that should not be ignored when working in data analytics or data science. While there are many ways to perform data validation in R, this talk will explore Rich Iannone and Mauricio Vargas’ pointblank package for this purpose.\n\n\n\n\n\n\nOct 26, 2022\n\n\nCleveland R User Group\n\n\n\n\n\n\n  \n\n\n\n\nCollages and Patterns: Making Art in R\n\n\n\n\n\nHave you just started creating generative art in R, or are you interested in starting? Have you tried to start the process but couldn’t wrap your head around how to do it? If so, “Collages and Patterns: Making art in R”, might be of interest!\n\n\n\n\n\n\nOct 12, 2022\n\n\nR-Ladies Philly\n\n\n\n\n\n\n  \n\n\n\n\nMaking Data Pipelines in R: A Story from a Self-Taught Perspective\n\n\n\n\n\nWhen people first learn about R’s capabilities to create fully integrated systems, automated visuals, and seamless data pipelines, the reaction can span from disbelief to amazement…\n\n\n\n\n\n\nJul 27, 2022\n\n\nRStudio::conf(2022)\n\n\n\n\n\n\n  \n\n\n\n\nMaking Functions For Rtistry\n\n\n\n\n\nThe Functions in Rtistry workshop will dive into using functions to create, add variation to, and streamline generative art pieces made in the R language…\n\n\n\n\n\n\nMay 24, 2022\n\n\nR-Ladies Johannesburg\n\n\n\n\n\n\n  \n\n\n\n\nAn Introduction to Rtistry Using ggplot2 in R\n\n\n\n\n\nIf you’ve ever seen pictures of generative art made in R and wondered if you could ever create art like that, the answer is “Yes.”…\n\n\n\n\n\n\nJan 13, 2022\n\n\nTunis R User Group\n\n\n\n\n\n\nNo matching items"
  }
]